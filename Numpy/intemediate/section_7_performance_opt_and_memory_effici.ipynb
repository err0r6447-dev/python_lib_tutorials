{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° Section 7: Performance Optimization and Memory Efficiency\n",
    "\n",
    "NumPy is built for speed ‚Äî but writing fast NumPy code means understanding how it stores data, manages memory, and leverages vectorization.\n",
    "\n",
    "This section explores performance profiling, in-place operations, broadcasting efficiency, and practical ways to minimize unnecessary memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† 1. Why NumPy Is Fast\n",
    "\n",
    "NumPy‚Äôs performance comes from two main ideas:\n",
    "- **Vectorization:** loops are pushed down to C, reducing Python overhead.\n",
    "- **Contiguous memory layout:** arrays are stored in blocks of memory that CPU caches can process efficiently.\n",
    "\n",
    "Let‚Äôs compare a Python loop to a vectorized NumPy operation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "%timeit [i ** 2 for i in range(10_000_000)]  # Pure Python loop\n",
    "%timeit np.arange(10_000_000) ** 2          # Vectorized NumPy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚û°Ô∏è The vectorized version runs **tens to hundreds of times faster** because it uses C loops internally.\n",
    "\n",
    "This difference grows dramatically as array size increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© 2. In-place Operations\n",
    "\n",
    "Creating new arrays consumes both memory and CPU time. Instead, we can perform operations *in place* whenever possible.\n",
    "\n",
    "Use operators like `+=`, `*=`, and slicing assignments to modify arrays without allocating new memory."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example: in-place scaling\n",
    "data = np.arange(1_000_000, dtype=np.float64)\n",
    "print(\"Before scaling:\", data[:5])\n",
    "\n",
    "# Normal (allocates new array)\n",
    "data_scaled = data * 1.2\n",
    "\n",
    "# In-place operation (no new array created)\n",
    "data *= 1.2\n",
    "print(\"After in-place scaling:\", data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify whether two arrays share the same memory using `np.may_share_memory()` or `np.shares_memory()`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "a = np.arange(10)\n",
    "b = a[::2]  # Every second element ‚Äî view, not copy\n",
    "\n",
    "print(\"Shares memory:\", np.shares_memory(a, b))\n",
    "print(\"May share memory:\", np.may_share_memory(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 3. Measuring Performance with `%timeit` and `np.benchmark`\n",
    "\n",
    "The `%timeit` magic command measures execution time accurately by running multiple iterations.\n",
    "For memory profiling, you can combine it with `sys.getsizeof()` or third-party profilers like `memory_profiler`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "\n",
    "arr = np.arange(1_000_000)\n",
    "print(\"Array size (bytes):\", arr.nbytes)\n",
    "print(\"Python object overhead (bytes):\", sys.getsizeof(arr))\n",
    "\n",
    "# Compare slicing vs. copy\n",
    "%timeit arr[1000:2000]         # View (fast, no copy)\n",
    "%timeit arr[1000:2000].copy()  # Explicit copy (slower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ 4. Memory Mapping Large Datasets\n",
    "\n",
    "When dealing with large arrays that don't fit in RAM, **memory mapping** lets you work with data on disk as if it were in memory.\n",
    "\n",
    "Use `np.memmap()` to load subsets of huge arrays efficiently."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# Create a large array and save to disk\n",
    "filename = 'large_array.dat'\n",
    "if not os.path.exists(filename):\n",
    "    arr = np.arange(10_000_000, dtype=np.float32)\n",
    "    arr.tofile(filename)\n",
    "\n",
    "# Load using memory mapping (read-only mode)\n",
    "mmap_arr = np.memmap(filename, dtype=np.float32, mode='r', shape=(10_000_000,))\n",
    "\n",
    "print(\"First 5 elements:\", mmap_arr[:5])\n",
    "print(\"Memory-mapped array type:\", type(mmap_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ 5. Vectorization vs. Python Loops ‚Äî Practical Example\n",
    "\n",
    "Suppose you need to compute the Euclidean distance between 100,000 random points and the origin.\n",
    "Let's compare a loop vs. a vectorized solution."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "N = 100_000\n",
    "points = rng.random((N, 3)) * 10  # 3D coordinates\n",
    "\n",
    "# Python loop (slow)\n",
    "def loop_distance(pts):\n",
    "    out = []\n",
    "    for p in pts:\n",
    "        out.append(np.sqrt(p[0]**2 + p[1]**2 + p[2]**2))\n",
    "    return np.array(out)\n",
    "\n",
    "%timeit loop_distance(points)\n",
    "\n",
    "# Vectorized (fast)\n",
    "%timeit np.sqrt(np.sum(points**2, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚û°Ô∏è The vectorized solution is typically **100√ó faster**, cleaner, and memory-efficient.\n",
    "\n",
    "Whenever possible, push operations down to NumPy‚Äôs C-level routines rather than writing Python loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Under the Hood: How NumPy Manages Memory\n",
    "\n",
    "- Arrays are stored in **contiguous memory blocks**, either C-order (row-major) or Fortran-order (column-major).\n",
    "- Slicing creates **views** (not copies) that reference the same buffer.\n",
    "- Operations that require reordering (e.g., transposing non-contiguous arrays) trigger new allocations.\n",
    "- `strides` determine how many bytes to step in memory for each axis.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "arr = np.arange(9).reshape(3, 3)\n",
    "print(arr.strides)\n",
    "```\n",
    "Each stride is the byte-step to move between rows/columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Best Practices & Pitfalls\n",
    "\n",
    "‚úî Always prefer **vectorized** operations over Python loops.\n",
    "‚úî Use **in-place operations** to save memory when possible.\n",
    "‚úî Use **slicing** to create views instead of copies.\n",
    "‚úî Check **array flags** (`arr.flags`) to confirm memory layout.\n",
    "‚úî For massive datasets, use **`np.memmap()`** or chunking.\n",
    "\n",
    "**Common pitfalls:**\n",
    "- Forgetting that some operations (like transpose of non-contiguous data) trigger copies.\n",
    "- Repeatedly allocating large temporary arrays.\n",
    "- Mixing different dtypes ‚Äî causes implicit upcasting and extra memory use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí™ Challenge Exercise\n",
    "\n",
    "**Task:**\n",
    "Create a 1,000,000-element float array. Compute its z-score (standardization) using an *in-place* approach ‚Äî i.e., modify the array without creating a new one.\n",
    "\n",
    "*Hint:* z-score = (x - mean) / std\n",
    "\n",
    "```python\n",
    "# Your code here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- End of Section 7 ‚Äî Continue to Section 8 ---\n",
    "\n",
    "Next, we'll explore **Integrating NumPy with Other Libraries**, where you'll learn how NumPy arrays interact seamlessly with pandas, scikit-learn, and Numba for high-performance analytics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
