{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Section 8: Memory Mapping, Shared Arrays, and Performance Profiling\n",
    "\n",
    "As datasets grow into gigabytes or terabytes, you can‚Äôt always load everything into memory at once. NumPy provides advanced tools for handling **large arrays efficiently**, especially for:\n",
    "- **Memory mapping** (loading large data from disk without fully reading it into memory)\n",
    "- **Shared memory arrays** (allowing multiple processes to share data efficiently)\n",
    "- **Performance profiling** (measuring and optimizing speed and memory usage)\n",
    "\n",
    "In this section, you‚Äôll:\n",
    "1. Work with `np.memmap` to stream huge datasets.\n",
    "2. Use `multiprocessing` with shared memory arrays for parallel computation.\n",
    "3. Apply profiling techniques (`%timeit`, `tracemalloc`, `np.benchmark`) to optimize performance.\n",
    "4. Explore real-world applications like **satellite image processing** and **sensor data pipelines**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ 1. Memory Mapping with `np.memmap`\n",
    "\n",
    "Memory mapping allows you to access parts of a file as if it were a NumPy array, without fully loading it into memory. This is crucial for working with large binary datasets ‚Äî for example, climate, genomics, or image sensor data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Simulate a large dataset (e.g., satellite pixel data)\n",
    "filename = 'large_satellite_data.dat'\n",
    "shape = (5000, 5000)  # 25 million pixels\n",
    "\n",
    "# Create the file using memmap and fill with simulated data\n",
    "if not os.path.exists(filename):\n",
    "    data = np.memmap(filename, dtype='float32', mode='w+', shape=shape)\n",
    "    data[:] = np.random.random(shape)\n",
    "    del data  # Flush to disk\n",
    "\n",
    "# Now reopen it in read-only mode\n",
    "mapped_data = np.memmap(filename, dtype='float32', mode='r', shape=shape)\n",
    "\n",
    "# Access a small part efficiently (no full load)\n",
    "sample = mapped_data[1000:1010, 1000:1010]\n",
    "print(\"Sample block:\\n\", np.round(sample, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ The file `large_satellite_data.dat` could be several GBs ‚Äî but `np.memmap` only reads the parts you access, on demand.\n",
    "\n",
    "This is extremely useful for **out-of-core processing** ‚Äî working with data larger than RAM (e.g., satellite images, MRI scans, or time-series logs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 2. Real-World Example: Streaming Sensor Data\n",
    "\n",
    "Imagine a factory producing high-frequency sensor readings ‚Äî temperature, vibration, and pressure ‚Äî every millisecond. Instead of loading all data, you can map the file and process it in blocks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Simulate memory-mapped sensor data file\n",
    "sensor_file = 'sensor_data.dat'\n",
    "n_rows = 10_000_000  # 10 million readings\n",
    "\n",
    "# Create synthetic data file if not exists\n",
    "if not os.path.exists(sensor_file):\n",
    "    sensors = np.memmap(sensor_file, dtype='float32', mode='w+', shape=(n_rows, 3))\n",
    "    sensors[:] = np.random.normal(loc=[25, 0.1, 100], scale=[2, 0.05, 10], size=(n_rows, 3))\n",
    "    del sensors\n",
    "\n",
    "# Map and process in chunks\n",
    "sensor_data = np.memmap(sensor_file, dtype='float32', mode='r', shape=(n_rows, 3))\n",
    "\n",
    "batch_size = 1_000_000\n",
    "means = []\n",
    "for start in range(0, n_rows, batch_size):\n",
    "    block = sensor_data[start:start + batch_size]\n",
    "    means.append(block.mean(axis=0))\n",
    "\n",
    "print(\"Mean sensor values (temperature, vibration, pressure):\")\n",
    "print(np.round(np.mean(means, axis=0), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Only one batch at a time is loaded into memory, allowing efficient computation on **huge sensor datasets** that would otherwise exceed system RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© 3. Shared Memory Arrays for Multiprocessing\n",
    "\n",
    "When you use Python‚Äôs `multiprocessing`, each process has its own memory space ‚Äî duplicating large arrays wastes RAM.\n",
    "\n",
    "NumPy arrays can be **shared across processes** using `multiprocessing.shared_memory`, avoiding redundant copies and making parallel computation fast and memory-efficient."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from multiprocessing import shared_memory, Process\n",
    "\n",
    "# Create shared memory NumPy array\n",
    "shape = (10_000_000,)\n",
    "data = np.random.random(shape)\n",
    "\n",
    "shm = shared_memory.SharedMemory(create=True, size=data.nbytes)\n",
    "shared_arr = np.ndarray(shape, dtype=data.dtype, buffer=shm.buf)\n",
    "shared_arr[:] = data[:]\n",
    "\n",
    "# Worker function operating on shared array\n",
    "def worker(start, end, name):\n",
    "    shm = shared_memory.SharedMemory(name=name)\n",
    "    arr = np.ndarray(shape, dtype=np.float64, buffer=shm.buf)\n",
    "    arr[start:end] = np.sqrt(arr[start:end])  # Example transformation\n",
    "    shm.close()\n",
    "\n",
    "# Run parallel workers\n",
    "chunk = len(shared_arr) // 4\n",
    "processes = [Process(target=worker, args=(i*chunk, (i+1)*chunk, shm.name)) for i in range(4)]\n",
    "for p in processes: p.start()\n",
    "for p in processes: p.join()\n",
    "\n",
    "print(\"Shared array processed by 4 workers in parallel.\")\n",
    "shm.close(); shm.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ This technique allows **true parallel processing** of large arrays without data duplication ‚Äî ideal for **image preprocessing**, **financial Monte Carlo simulations**, or **scientific modeling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ 4. Performance Profiling\n",
    "\n",
    "NumPy is fast, but optimizing your pipeline often requires measuring performance precisely.\n",
    "\n",
    "You can profile your operations using:\n",
    "- `%%timeit` (Jupyter magic command)\n",
    "- `time` module (basic timing)\n",
    "- `tracemalloc` (memory tracking)\n",
    "- `np.benchmark` or `perf_counter` (for advanced use)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import tracemalloc, time\n",
    "\n",
    "# Example: comparing two methods for normalization\n",
    "x = np.random.random(10_000_000)\n",
    "\n",
    "tracemalloc.start()\n",
    "start = time.perf_counter()\n",
    "x_norm1 = (x - np.mean(x)) / np.std(x)\n",
    "mem1, _ = tracemalloc.get_traced_memory()\n",
    "time1 = time.perf_counter() - start\n",
    "\n",
    "tracemalloc.reset_peak()\n",
    "start = time.perf_counter()\n",
    "x_norm2 = (x - x.min()) / (x.max() - x.min())\n",
    "mem2, _ = tracemalloc.get_traced_memory()\n",
    "time2 = time.perf_counter() - start\n",
    "tracemalloc.stop()\n",
    "\n",
    "print(f\"Z-score normalization: {time1:.4f}s, {mem1/1e6:.2f} MB\")\n",
    "print(f\"Min-max normalization: {time2:.4f}s, {mem2/1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîç Profiling helps identify bottlenecks and memory peaks. You can then apply techniques like:\n",
    "- Using `out=` parameters to avoid unnecessary copies\n",
    "- Processing data in blocks\n",
    "- Leveraging compiled functions (Numba, Cython)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç 5. Real-World Example: Climate Data Aggregation Pipeline\n",
    "\n",
    "Let‚Äôs simulate a scenario where we process large temperature grids across multiple months.\n",
    "\n",
    "We‚Äôll use `memmap` to stream the data and compute monthly means without loading everything into RAM."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "n_days, n_lat, n_lon = 365, 180, 360\n",
    "climate_file = 'climate_temp_data.dat'\n",
    "\n",
    "# Create large climate dataset if missing (180x360 grid for 365 days)\n",
    "if not os.path.exists(climate_file):\n",
    "    data = np.memmap(climate_file, dtype='float32', mode='w+', shape=(n_days, n_lat, n_lon))\n",
    "    data[:] = np.random.normal(loc=15, scale=10, size=(n_days, n_lat, n_lon))\n",
    "    del data\n",
    "\n",
    "# Map file and compute monthly means\n",
    "mapped = np.memmap(climate_file, dtype='float32', mode='r', shape=(n_days, n_lat, n_lon))\n",
    "month_means = []\n",
    "for month in range(12):\n",
    "    start, end = month * 30, min((month + 1) * 30, n_days)\n",
    "    month_means.append(mapped[start:end].mean())\n",
    "\n",
    "print(\"Monthly average temperatures (¬∞C):\")\n",
    "print(np.round(month_means, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ This mimics **climate model post-processing**, a common task in Earth science where global temperature grids can be **hundreds of GBs**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Summary\n",
    "\n",
    "- `np.memmap` allows streaming large data directly from disk.\n",
    "- `multiprocessing.shared_memory` lets you share large arrays efficiently between processes.\n",
    "- Profiling (`time`, `tracemalloc`, `timeit`) helps detect bottlenecks.\n",
    "- Real-world applications include **sensor analytics**, **climate modeling**, **image processing**, and **financial simulations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Challenge Exercise\n",
    "\n",
    "1. Create a 10 GB file-backed array using `np.memmap`.\n",
    "2. Process it in 100 MB chunks, computing a running average.\n",
    "3. Measure memory use and runtime with `tracemalloc`.\n",
    "4. Compare the results with a fully loaded in-memory version.\n",
    "\n",
    "*Goal:* Understand how streaming computation saves memory and scales with dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- End of Section 8 ---\n",
    "\n",
    "Next up ‚Üí **Section 9: Advanced Linear Algebra, Eigenvalues, and Decompositions**\n",
    "\n",
    "We‚Äôll dive into matrix factorization techniques (SVD, PCA, eigen-decomposition) ‚Äî essential for machine learning and scientific computing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
