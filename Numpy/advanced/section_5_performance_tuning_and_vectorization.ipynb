{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f971d65",
   "metadata": {},
   "source": [
    "# Section 5 ‚Äî Performance Tuning and Vectorization Strategies\n",
    "\n",
    "NumPy‚Äôs biggest strength lies in its **vectorized operations** ‚Äî executing array computations at compiled C speed.  \n",
    "However, writing *truly efficient* NumPy code requires understanding **how memory layout, data types, and vectorization** interact.\n",
    "\n",
    "In this section, you‚Äôll learn:\n",
    "- How to identify performance bottlenecks in NumPy code.\n",
    "- Techniques to replace slow Python loops with vectorized alternatives.\n",
    "- The impact of data types, memory layout, and broadcasting.\n",
    "- Practical comparisons with Python and other optimization tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fce8802",
   "metadata": {},
   "source": [
    "## 5.1 Why Vectorization Matters\n",
    "\n",
    "Vectorization refers to writing operations that act on **entire arrays** instead of element-by-element loops.  \n",
    "NumPy achieves this through C-level ufunc loops that are **orders of magnitude faster** than Python iteration.\n",
    "\n",
    "Let‚Äôs see how big the difference can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d12672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "size = 10_000_000\n",
    "x = np.random.rand(size)\n",
    "y = np.random.rand(size)\n",
    "\n",
    "# --- Python loop ---\n",
    "start = time.time()\n",
    "z_py = [a + b for a, b in zip(x, y)]\n",
    "print(f\"Python loop time: {time.time() - start:.4f} sec\")\n",
    "\n",
    "# --- NumPy vectorized ---\n",
    "start = time.time()\n",
    "z_np = x + y\n",
    "print(f\"NumPy vectorized time: {time.time() - start:.4f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b703a277",
   "metadata": {},
   "source": [
    "NumPy vectorization typically runs **50‚Äì100x faster** than pure Python loops, thanks to compiled C code and SIMD (Single Instruction, Multiple Data) optimizations.  \n",
    "\n",
    "This performance gain scales with data size and operation complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48f5f51",
   "metadata": {},
   "source": [
    "## 5.2 Profiling and Identifying Bottlenecks\n",
    "\n",
    "Before optimizing, always **profile** your code. NumPy performance issues often come from unnecessary copying, type conversions, or Python-level loops.\n",
    "\n",
    "We‚Äôll use `%timeit` and NumPy‚Äôs built-in memory profiling to identify inefficiencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a9e04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit x + y  # Fast vectorized addition\n",
    "\n",
    "def slow_square(arr):\n",
    "    out = []\n",
    "    for val in arr:\n",
    "        out.append(val ** 2)\n",
    "    return np.array(out)\n",
    "\n",
    "%timeit slow_square(x)\n",
    "\n",
    "# Fast alternative using vectorization\n",
    "%timeit x ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b384bd",
   "metadata": {},
   "source": [
    "Even for simple operations like squaring, the difference can be *thousands of times faster*.  \n",
    "\n",
    "Profiling helps confirm where Python loops or hidden type conversions are killing performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7048255e",
   "metadata": {},
   "source": [
    "## 5.3 Avoiding Temporary Arrays with `out=` Parameter\n",
    "\n",
    "Most NumPy operations allocate new arrays for results. When working with large datasets, this can waste memory and slow down your program.  \n",
    "\n",
    "Use the `out=` parameter to reuse existing memory and perform **in-place computations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda9adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(1e6)\n",
    "b = np.arange(1e6)\n",
    "res = np.empty_like(a)\n",
    "\n",
    "# Standard operation (allocates new array)\n",
    "%timeit c = a + b\n",
    "\n",
    "# In-place operation\n",
    "%timeit np.add(a, b, out=res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb5f9c8",
   "metadata": {},
   "source": [
    "Using `out=` avoids extra memory allocations and reduces CPU cache misses. This pattern is common in high-performance numerical code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d3005d",
   "metadata": {},
   "source": [
    "## 5.4 Efficient Data Types and Casting\n",
    "\n",
    "Data type (`dtype`) selection affects both speed and memory. Smaller dtypes use less memory and can be faster ‚Äî but may lose precision.  \n",
    "\n",
    "Always match the smallest dtype that still provides acceptable accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcfe80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x32 = np.random.rand(1_000_000).astype(np.float32)\n",
    "x64 = np.random.rand(1_000_000).astype(np.float64)\n",
    "\n",
    "%timeit x32 * 2.5\n",
    "%timeit x64 * 2.5\n",
    "\n",
    "print(\"Memory (float32):\", x32.nbytes / 1e6, \"MB\")\n",
    "print(\"Memory (float64):\", x64.nbytes / 1e6, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d6cb6e",
   "metadata": {},
   "source": [
    "Using `float32` often doubles performance in memory-bound operations ‚Äî but beware of cumulative precision loss in scientific applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad0526",
   "metadata": {},
   "source": [
    "## 5.5 Leveraging Broadcasting and Preallocation\n",
    "\n",
    "Repeatedly resizing or appending arrays is slow. Instead, **preallocate** memory and use broadcasting for efficient element-wise computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6e90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "a = np.random.rand(n, 1)\n",
    "b = np.random.rand(1, n)\n",
    "\n",
    "# Broadcasting creates a full grid efficiently\n",
    "distances = np.sqrt((a - b)**2)\n",
    "print(distances.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1613711",
   "metadata": {},
   "source": [
    "Broadcasting automatically expands smaller arrays without explicit loops, but be careful: the resulting temporary arrays can still consume a lot of memory.  \n",
    "Consider chunking or using libraries like **Dask** for extremely large arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8238110c",
   "metadata": {},
   "source": [
    "## 5.6 Under the Hood: Vectorization Mechanics\n",
    "\n",
    "Internally, NumPy vectorization works by:\n",
    "1. Compiling **C-level inner loops** for each operation and dtype combination.\n",
    "2. Using **SIMD instructions** to perform multiple arithmetic operations per CPU cycle.\n",
    "3. Minimizing Python function calls and overhead.\n",
    "\n",
    "This means the Python interpreter never touches individual elements ‚Äî the heavy lifting happens entirely in C, using optimized libraries (BLAS, LAPACK, or SIMD-accelerated loops)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9f72ef",
   "metadata": {},
   "source": [
    "## 5.7 Best Practices & Pitfalls\n",
    "\n",
    "**‚úÖ Best Practices:**\n",
    "- Always profile before optimizing.\n",
    "- Replace Python loops with ufuncs or broadcasting.\n",
    "- Use `out=` for in-place updates.\n",
    "- Match data types to task requirements.\n",
    "- Reuse allocated arrays instead of repeated concatenation.\n",
    "\n",
    "**‚ö†Ô∏è Pitfalls:**\n",
    "- Avoid growing arrays dynamically with `np.append` inside loops.\n",
    "- Beware of implicit dtype upcasting (e.g., mixing int and float).\n",
    "- Don‚Äôt assume broadcasting is free ‚Äî it may create large temporary arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdfc96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating array growth pitfall\n",
    "arr = np.array([])\n",
    "for i in range(1000):\n",
    "    arr = np.append(arr, i)  # Very slow\n",
    "\n",
    "# Better: preallocate\n",
    "arr_fast = np.empty(1000)\n",
    "for i in range(1000):\n",
    "    arr_fast[i] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2221490f",
   "metadata": {},
   "source": [
    "## üß© Challenge Exercise\n",
    "\n",
    "**Task:** Optimize a naive implementation of the Euclidean distance between two arrays.\n",
    "\n",
    "1. Write a pure Python loop version.\n",
    "2. Rewrite it using NumPy vectorization.\n",
    "3. Add a version using `out=` for in-place updates.\n",
    "4. Measure time and memory efficiency for all three versions.\n",
    "\n",
    "_Hint: Use arrays of size ‚â• 10‚Å∂ to see meaningful performance differences._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683a8859",
   "metadata": {},
   "source": [
    "---\n",
    "# --- End of Section 5 ‚Äî Continue to Section 6 ---\n",
    "In the next section, we‚Äôll explore **Memory Mapping and Large Dataset Handling**, learning how NumPy can work with data that doesn‚Äôt fit in memory efficiently using `np.memmap` and related tools."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
