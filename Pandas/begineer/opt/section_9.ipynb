{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß© Add-on Module 9: Pandas Integration with External Systems (SQL, Spark & APIs)\n",
    "\n",
    "In large-scale data workflows, Pandas rarely operates alone. It often integrates with:\n",
    "\n",
    "- Databases (MySQL, PostgreSQL, SQLite) via **SQLAlchemy**\n",
    "- Big Data engines like **Apache Spark**\n",
    "- **REST APIs** for ingestion and enrichment\n",
    "\n",
    "This module explores how Pandas connects, exchanges, and synchronizes data with external systems efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Integrating Pandas with SQL Databases\n",
    "\n",
    "Pandas supports SQL operations natively via **SQLAlchemy** ‚Äî a powerful ORM and database abstraction layer.\n",
    "\n",
    "You can read/write tables, run queries, and even join Pandas DataFrames with SQL tables."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create an in-memory SQLite database\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "\n",
    "# Sample DataFrame\n",
    "sales = pd.DataFrame({\n",
    "    'order_id': [101, 102, 103, 104],\n",
    "    'customer': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'amount': [250, 180, 600, 120]\n",
    "})\n",
    "\n",
    "# Write to SQL table\n",
    "sales.to_sql('sales_table', con=engine, index=False, if_exists='replace')\n",
    "\n",
    "# Read back using SQL query\n",
    "df_sql = pd.read_sql('SELECT * FROM sales_table WHERE amount > 200', con=engine)\n",
    "df_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Advanced: Parameterized Queries & Joins"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create another table\n",
    "customers = pd.DataFrame({\n",
    "    'customer': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'city': ['NY', 'Paris', 'Berlin', 'Delhi']\n",
    "})\n",
    "customers.to_sql('customers_table', con=engine, index=False, if_exists='replace')\n",
    "\n",
    "# Perform join via SQL\n",
    "query = '''\n",
    "SELECT s.order_id, s.customer, c.city, s.amount\n",
    "FROM sales_table AS s\n",
    "JOIN customers_table AS c ON s.customer = c.customer\n",
    "WHERE s.amount > :amt\n",
    "'''\n",
    "\n",
    "joined_df = pd.read_sql(query, con=engine, params={'amt': 150})\n",
    "joined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Working with REST APIs\n",
    "\n",
    "Pandas can easily ingest JSON responses from RESTful APIs ‚Äî transforming them into DataFrames for analysis.\n",
    "\n",
    "### Example: Fetching and Normalizing API Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "# Example API (placeholder JSON)\n",
    "url = 'https://jsonplaceholder.typicode.com/posts'\n",
    "response = requests.get(url)\n",
    "posts = pd.json_normalize(response.json())\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç API + Pandas Processing Workflow\n",
    "Use case: Combine API data with existing datasets for enrichment.\n",
    "\n",
    "For instance, enriching a transaction dataset with currency exchange rates or weather data from APIs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Simulate enrichment workflow\n",
    "transactions = pd.DataFrame({\n",
    "    'txn_id': [1, 2, 3],\n",
    "    'amount_usd': [200, 150, 350]\n",
    "})\n",
    "\n",
    "# Suppose API provides current USD ‚Üí EUR rate\n",
    "exchange_rate = 0.93  # Mocked API response\n",
    "transactions['amount_eur'] = transactions['amount_usd'] * exchange_rate\n",
    "transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Pandas with PySpark: Bridging Small and Big Data\n",
    "\n",
    "Pandas integrates seamlessly with **PySpark DataFrames** for distributed processing.\n",
    "\n",
    "Use this when scaling Pandas pipelines to terabytes of data while retaining Pandas-like syntax."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName('PandasIntegration').getOrCreate()\n",
    "\n",
    "# Convert Pandas ‚Üí Spark\n",
    "spark_df = spark.createDataFrame(sales)\n",
    "\n",
    "# Spark transformations\n",
    "spark_df.createOrReplaceTempView('sales')\n",
    "spark_result = spark.sql('SELECT customer, SUM(amount) as total_spent FROM sales GROUP BY customer')\n",
    "\n",
    "# Convert back to Pandas\n",
    "pandas_result = spark_result.toPandas()\n",
    "pandas_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Real-World Problem 1: Data Warehouse ETL Pipeline\n",
    "\n",
    "**Scenario:**\n",
    "- Load millions of order records from PostgreSQL\n",
    "- Clean, transform, and aggregate them in Pandas\n",
    "- Write the summary back to SQL for reporting\n",
    "\n",
    "**Workflow:**\n",
    "```text\n",
    "PostgreSQL ‚Üí SQLAlchemy ‚Üí Pandas Cleaning ‚Üí Pandas GroupBy ‚Üí SQL Write-Back\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Real-World Problem 2: API Data Enrichment for E-Commerce\n",
    "\n",
    "**Scenario:**\n",
    "- Fetch product reviews from an external API.\n",
    "- Merge API sentiment scores with internal sales.\n",
    "- Generate a performance report combining both sources.\n",
    "\n",
    "**Approach:**\n",
    "- Use `requests` + `json_normalize()` to flatten API JSON.\n",
    "- Merge on `product_id`.\n",
    "- Save the merged DataFrame to Parquet for fast analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Under the Hood\n",
    "\n",
    "- **SQLAlchemy** translates Python operations to SQL syntax.\n",
    "- **Pandas** uses `pyarrow` for efficient Arrow-based transfers.\n",
    "- **Spark Integration** uses Arrow serialization via `toPandas()` and `createDataFrame()`.\n",
    "- **REST APIs** are JSON-based and flattened into tabular form using `json_normalize()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Best Practices\n",
    "\n",
    "- Always use parameterized queries for security.\n",
    "- Prefer **Parquet** or **Arrow** when exchanging large data.\n",
    "- Use **connection pooling** in production for SQL.\n",
    "- Cache API responses when possible to reduce latency.\n",
    "- Benchmark I/O performance using `%timeit` or `perf_counter`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Challenge Exercise\n",
    "\n",
    "You work at a logistics company managing millions of shipments.\n",
    "\n",
    "1. Fetch live shipment status from a REST API.\n",
    "2. Merge it with SQL-based shipment data in Pandas.\n",
    "3. Transform timestamps to local timezones using Pandas `dt`.\n",
    "4. Write the combined DataFrame to Parquet.\n",
    "5. Push summary metrics (on-time %, delayed %, avg delivery time) back to the SQL database."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
