{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Add-on Module 4: Profiling, Benchmarking & Scaling Pandas Pipelines\n",
    "\n",
    "In this final advanced section, we focus on **diagnosing and improving Pandas performance**.\n",
    "You'll learn to:\n",
    "- Profile CPU, memory, and I/O usage.\n",
    "- Benchmark operations across alternative implementations.\n",
    "- Scale Pandas pipelines using chunking, Dask, and parallel tools.\n",
    "- Apply caching, lazy loading, and schema optimization for production pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Profiling Pandas Performance\n",
    "\n",
    "Before optimizing, you must identify **bottlenecks**. Pandas offers several ways to measure performance — from built-in timing to third-party profilers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Generate synthetic dataset\n",
    "df = pd.DataFrame({\n",
    "    'A': np.random.randint(1, 100, 1_000_000),\n",
    "    'B': np.random.randn(1_000_000)\n",
    "})\n",
    "\n",
    "# Example operation\n",
    "start = time.time()\n",
    "df['C'] = df['A'] * np.log1p(df['B'].abs())\n",
    "print(f\"Execution Time: {time.time() - start:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `%timeit` and `memory_usage()`\n",
    "For quick benchmarking in Jupyter, use `%timeit`. For deeper insights, track memory consumption."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%timeit df['A'] * np.log1p(df['B'].abs())\n",
    "print('Memory usage (MB):', df.memory_usage(deep=True).sum() / (1024**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Profiling with `line_profiler` and `memory_profiler`\n",
    "\n",
    "For detailed line-by-line CPU and memory analysis, use specialized profilers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install -q line_profiler memory_profiler\n",
    "from memory_profiler import memory_usage\n",
    "import numpy as np\n",
    "\n",
    "def compute(df):\n",
    "    df['scaled'] = (df['A'] ** 2 + np.sqrt(df['B'].abs())) / np.log1p(df['A'])\n",
    "    return df\n",
    "\n",
    "mem_before = memory_usage()[0]\n",
    "df = compute(df)\n",
    "mem_after = memory_usage()[0]\n",
    "print(f\"Memory delta: {mem_after - mem_before:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under the Hood\n",
    "- **line_profiler** measures per-line execution time.\n",
    "- **memory_profiler** hooks into Python’s garbage collector.\n",
    "- Pandas operations release the GIL in most numeric cases but not for Python objects (dtype=object)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benchmarking Strategies\n",
    "\n",
    "Systematically compare different implementations of a task — e.g., loops vs vectorization vs Dask."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Loop-based computation\n",
    "def loop_sum(df):\n",
    "    return [x + y for x, y in zip(df['A'], df['B'])]\n",
    "\n",
    "# Vectorized computation\n",
    "def vector_sum(df):\n",
    "    return df['A'] + df['B']\n",
    "\n",
    "# Dask-based computation\n",
    "def dask_sum(df):\n",
    "    ddf = dd.from_pandas(df, npartitions=8)\n",
    "    return ddf['A'] + ddf['B']\n",
    "\n",
    "# Timing comparison\n",
    "import time\n",
    "for func in [loop_sum, vector_sum, dask_sum]:\n",
    "    start = time.time()\n",
    "    _ = func(df)\n",
    "    print(f\"{func.__name__:<12} : {time.time() - start:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Observation:** Vectorized and Dask-based implementations outperform explicit loops dramatically.\n",
    "\n",
    "**Tip:** Always benchmark using realistic data volumes and compute environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scaling Patterns\n",
    "\n",
    "Scaling Pandas involves either **vertical scaling (optimization)** or **horizontal scaling (distribution)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Chunk Processing (Streaming Large Files)\n",
    "For files larger than memory, load data in chunks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "chunk_iter = pd.read_csv('large_data.csv', chunksize=500_000)\n",
    "aggregated = []\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    summary = chunk.groupby('category')['value'].mean()\n",
    "    aggregated.append(summary)\n",
    "\n",
    "final_df = pd.concat(aggregated)\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Parallel Execution with Swifter / Modin\n",
    "Distribute `apply()` across cores for compute-heavy transforms."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install -q swifter\n",
    "import swifter\n",
    "\n",
    "def complex_transform(x):\n",
    "    return np.sin(x) * np.log1p(abs(x))\n",
    "\n",
    "df['transformed'] = df['B'].swifter.apply(complex_transform)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Dask for Cluster-scale Scaling\n",
    "Dask extends Pandas syntax to distributed computing with minimal changes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ddf = dd.from_pandas(df, npartitions=8)\n",
    "result = ddf.groupby('A')['B'].mean().compute()\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real-World Problem Examples\n",
    "\n",
    "### Problem 1: Benchmarking ETL Pipeline\n",
    "You’re tasked to migrate a legacy data-cleaning script. Measure performance difference:\n",
    "1. Original loop-based version.\n",
    "2. Vectorized Pandas version.\n",
    "3. Dask parallelized version.\n",
    "\n",
    "Collect time and memory profiles using `timeit` and `memory_profiler`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Scaling a Daily Aggregation Job\n",
    "An e-commerce dataset (10GB) must produce daily revenue summaries.\n",
    "Use **chunking + Dask** to scale the process incrementally.\n",
    "\n",
    "```python\n",
    "chunks = pd.read_csv('orders.csv', chunksize=1_000_000)\n",
    "for c in chunks:\n",
    "    daily = c.groupby('date')['revenue'].sum()\n",
    "    daily.to_csv('daily_summaries.csv', mode='a')\n",
    "```\n",
    "\n",
    "This minimizes memory footprint and improves throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices / Pitfalls\n",
    "\n",
    "✅ **Best Practices:**\n",
    "- Always **profile first**, optimize later.\n",
    "- Use **vectorization** as your default pattern.\n",
    "- Apply **Dask or Swifter** when memory or CPU is the limiting factor.\n",
    "- Cache intermediate results if reused across stages.\n",
    "\n",
    "⚠️ **Pitfalls:**\n",
    "- Excessive chunk sizes can overload memory.\n",
    "- Dask computations require `.compute()` — forgetting it leaves graphs unexecuted.\n",
    "- Profilers add overhead; use them for diagnosis, not production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Exercise\n",
    "\n",
    "**Task:** Build a profiling dashboard that:**\n",
    "1. Loads 2M synthetic records.\n",
    "2. Runs 3 operations (merge, apply, groupby).\n",
    "3. Profiles time and memory for each version — Pandas vs Dask.\n",
    "4. Summarize results as a performance comparison DataFrame.\n",
    "\n",
    "_Hint_: Use `%timeit`, `memory_usage()`, and Dask partitions.\n",
    "\n",
    "# --- End of Add-on Module Section 4 ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
