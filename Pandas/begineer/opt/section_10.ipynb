{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ Add-on Module 10 â€“ End-to-End Data Engineering Project\n",
    "\n",
    "## Objective\n",
    "Bring together everything learned across prior modules to design and implement a **real-world ETL (Extract-Transform-Load)** workflow using Pandas.\n",
    "\n",
    "This project simulates a retail company consolidating sales data from multiple sources (SQL database, REST API, and CSV files) and delivering analytical insights.\n",
    "\n",
    "**Core Concepts**  \n",
    "â†’ Efficient extraction and merging from heterogeneous data sources  \n",
    "â†’ Data cleaning, enrichment, and aggregation  \n",
    "â†’ Performance optimization and storage back-end integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Extract â€“ Loading Data from Multiple Sources"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import requests, io\n",
    "\n",
    "# Simulated SQL source\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "sales_sql = pd.DataFrame({\n",
    "    'order_id': [1, 2, 3, 4],\n",
    "    'product_id': [101, 102, 103, 104],\n",
    "    'quantity': [2, 1, 4, 3],\n",
    "    'price': [100, 250, 50, 300]\n",
    "})\n",
    "sales_sql.to_sql('sales', con=engine, index=False)\n",
    "sales_db = pd.read_sql('SELECT * FROM sales', con=engine)\n",
    "\n",
    "# Simulated REST API source\n",
    "url = 'https://jsonplaceholder.typicode.com/users'\n",
    "api_data = requests.get(url).json()\n",
    "customers_api = pd.json_normalize(api_data)[['id', 'name', 'address.city']].rename(columns={'id':'customer_id'})\n",
    "\n",
    "# Simulated CSV source (inventory)\n",
    "csv_data = io.StringIO('product_id,stock\\n101,20\\n102,5\\n103,50\\n104,0')\n",
    "inventory = pd.read_csv(csv_data)\n",
    "\n",
    "sales_db.head(), customers_api.head(), inventory.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Transform â€“ Data Cleaning & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compute total sale value\n",
    "sales_db['total_value'] = sales_db['quantity'] * sales_db['price']\n",
    "\n",
    "# Simulate adding a customer column (for join)\n",
    "sales_db['customer_id'] = [1,2,3,4]\n",
    "\n",
    "# Merge all sources\n",
    "merged = sales_db.merge(customers_api, on='customer_id', how='left').merge(inventory, on='product_id', how='left')\n",
    "\n",
    "# Handle missing stock\n",
    "merged['stock'] = merged['stock'].fillna(0)\n",
    "\n",
    "# Add new feature: stock status\n",
    "merged['in_stock'] = merged['stock'] > 0\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Enrich â€“ Add External Information (e.g., Currency Rates)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Mock API for currency conversion\n",
    "eur_rate = 0.93  # suppose we fetched this from exchangerate API\n",
    "merged['total_value_eur'] = merged['total_value'] * eur_rate\n",
    "merged[['order_id','name','total_value','total_value_eur']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Analyze â€“ Aggregation & Insights"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compute total sales by city\n",
    "city_sales = merged.groupby('address.city')['total_value'].sum().reset_index().rename(columns={'address.city':'city'})\n",
    "\n",
    "# Top products by revenue\n",
    "product_perf = merged.groupby('product_id')['total_value'].sum().sort_values(ascending=False)\n",
    "\n",
    "print(city_sales)  # summary by city\n",
    "print('\\nTop products by revenue:')\n",
    "print(product_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Load â€“ Store Results to Database and Parquet"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write aggregates back to SQL\n",
    "city_sales.to_sql('city_sales_summary', con=engine, index=False, if_exists='replace')\n",
    "\n",
    "# Save for analytics\n",
    "merged.to_parquet('sales_enriched.parquet', index=False)\n",
    "print('âœ… Data written to database and Parquet file successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Performance Optimization Tips"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Vectorized operations\n",
    "import numpy as np\n",
    "merged['discount'] = np.where(merged['total_value'] > 400, 0.1, 0.05)\n",
    "\n",
    "# Memory usage profiling\n",
    "print('Memory usage (MB):', merged.memory_usage(deep=True).sum() / 1e6)\n",
    "\n",
    "# Use categoricals for high-cardinality columns\n",
    "merged['address.city'] = merged['address.city'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§© Real-World Problem 1 â€“ Retail Sales Monitoring\n",
    "\n",
    "**Scenario:** Monitor daily sales from multiple warehouses and update a dashboard in real time.\n",
    "\n",
    "**Steps:**\n",
    "1. Fetch sales data from API every hour.  \n",
    "2. Store it in SQL via `to_sql()`.  \n",
    "3. Run a Pandas job to aggregate and visualize daily KPIs (e.g., total sales, average order value, out-of-stock items)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§© Real-World Problem 2 â€“ Customer 360Â° Profile Integration\n",
    "\n",
    "**Scenario:** Merge data from CRM API, sales database, and marketing CSV to create a unified customer profile.\n",
    "\n",
    "**Approach:**\n",
    "- Join on customer IDs across sources.  \n",
    "- Derive features like recency, frequency, monetary value (RFM).  \n",
    "- Export profiles to Parquet or a warehouse for ML use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Under the Hood\n",
    "- **SQLAlchemy** manages database connections and transaction safety.  \n",
    "- **Pandas I/O API** handles CSV/JSON/SQL adapters through modular backends.  \n",
    "- **Parquet Engine (PyArrow)** enables columnar compression and zero-copy read paths.  \n",
    "- **NumPy Vectorization** reduces Python loops in transform steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Best Practices\n",
    "- Validate schema consistency before merges.  \n",
    "- Automate extract/transform/load jobs with Airflow or Prefect.  \n",
    "- Profile pipelines with `cProfile` or `line_profiler`.  \n",
    "- Use `.query()` and `.assign()` for clear, chainable transforms.  \n",
    "- Compress large files using Snappy or ZSTD in Parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Challenge Exercise\n",
    "\n",
    "Design an automated ETL system that runs daily to:\n",
    "1. Pull customer and transaction data from two APIs.  \n",
    "2. Store raw data in a PostgreSQL staging area.  \n",
    "3. Transform it using Pandas (handling missing values, date normalization, and currency conversion).  \n",
    "4. Write clean data to an analytics database and a Parquet data lake.  \n",
    "5. Generate daily KPIs and email them as an HTML report to stakeholders."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
