{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Add-on Module: Parallelization, Vectorization & Lazy Evaluation in Pandas\n",
    "\n",
    "In this section, we dive into how **Pandas operations can be accelerated** through:\n",
    "- **Vectorization** using NumPy operations\n",
    "- **Parallelization** with libraries like `swifter` and `modin`\n",
    "- **Lazy evaluation** using `dask.dataframe` for large datasets\n",
    "\n",
    "You'll learn how to handle large-scale data pipelines efficiently, while understanding the tradeoffs between memory, performance, and execution control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vectorization in Pandas\n",
    "\n",
    "Vectorization allows operations to be applied over arrays without explicit loops. This uses optimized C-level implementations inside NumPy and Pandas."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Create synthetic dataset\n",
    "df = pd.DataFrame({\n",
    "    'A': np.random.randint(1, 100, 1_000_000),\n",
    "    'B': np.random.randint(1, 100, 1_000_000)\n",
    "})\n",
    "\n",
    "# Non-vectorized operation (slow)\n",
    "start = time.time()\n",
    "df['C_loop'] = [a * b for a, b in zip(df['A'], df['B'])]\n",
    "loop_time = time.time() - start\n",
    "\n",
    "# Vectorized operation (fast)\n",
    "start = time.time()\n",
    "df['C_vec'] = df['A'] * df['B']\n",
    "vec_time = time.time() - start\n",
    "\n",
    "print(f\"Loop time: {loop_time:.3f}s | Vectorized time: {vec_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Observation:** Vectorized code is **10–100× faster** and uses internal C loops.\n",
    "\n",
    "### Under the Hood: Why Vectorization Wins\n",
    "- Operates directly on **NumPy ndarray buffers**.\n",
    "- Uses **SIMD (Single Instruction, Multiple Data)** CPU instructions.\n",
    "- Avoids Python's GIL by executing C extensions natively.\n",
    "\n",
    "Vectorization is the foundation of high-performance data processing in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parallelization using Swifter & Modin\n",
    "\n",
    "When your functions are not easily vectorizable (like complex Python logic), libraries such as **Swifter** and **Modin** distribute Pandas operations across multiple CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example: Accelerating apply() with Swifter\n",
    "!pip install -q swifter\n",
    "import swifter\n",
    "\n",
    "# Apply custom function\n",
    "def complex_operation(x):\n",
    "    return np.sqrt(x**2 + np.log1p(x))\n",
    "\n",
    "# Using swifter to parallelize apply()\n",
    "df['D'] = df['A'].swifter.apply(complex_operation)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-World Problem 1: Parallel Customer Scoring\n",
    "\n",
    "**Scenario:** You have millions of customer transactions and need to compute a risk score using a non-vectorizable function.\n",
    "\n",
    "```python\n",
    "def risk_score(amount, frequency):\n",
    "    return np.log1p(amount) * np.sqrt(frequency)\n",
    "\n",
    "transactions['risk'] = transactions.swifter.apply(lambda row: risk_score(row['amount'], row['frequency']), axis=1)\n",
    "```\n",
    "\n",
    "⏱ Swifter automatically parallelizes across available cores for faster results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lazy Evaluation with Dask DataFrame\n",
    "\n",
    "Lazy evaluation means operations are **queued** until you explicitly trigger computation. This helps process datasets larger than memory by chunking them into partitions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install -q dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Create a Dask DataFrame\n",
    "dask_df = dd.from_pandas(df, npartitions=8)\n",
    "\n",
    "# Lazy operations (no immediate execution)\n",
    "result = (dask_df['A'] + dask_df['B']).mean()\n",
    "\n",
    "# Trigger computation\n",
    "print(result.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-World Problem 2: Processing a 5GB CSV File\n",
    "\n",
    "**Scenario:** You have a 5GB CSV file that doesn't fit in memory. Using Dask allows chunked reading and lazy evaluation.\n",
    "\n",
    "```python\n",
    "import dask.dataframe as dd\n",
    "big_df = dd.read_csv('sales_data_2024.csv')\n",
    "filtered = big_df[big_df['revenue'] > 10000]\n",
    "avg = filtered['revenue'].mean()\n",
    "print(avg.compute())  # Executes only when needed\n",
    "```\n",
    "\n",
    "✅ Efficient for large-scale ETL and analytics pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Profiling and Benchmarking\n",
    "\n",
    "Let's measure and compare performance using built-in tools like `%timeit` and `memory_usage()`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare vectorized vs. swifter performance\n",
    "%timeit df['A'] * df['B']\n",
    "%timeit df['A'].swifter.apply(complex_operation)\n",
    "\n",
    "# Memory usage estimation\n",
    "print(df.memory_usage(deep=True).sum() / (1024**2), 'MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under the Hood\n",
    "- **Dask:** Divides DataFrame into smaller Pandas partitions and uses a task graph scheduler.\n",
    "- **Swifter:** Uses `pandarallel` or `dask` under the hood depending on context.\n",
    "- **NumExpr:** Can further optimize numerical expressions with JIT compilation.\n",
    "- **Modin:** Distributes Pandas operations seamlessly across CPU cores or Ray/Dask clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices & Pitfalls\n",
    "\n",
    "✅ **Do:**\n",
    "- Prefer vectorized operations whenever possible.\n",
    "- Profile both time and memory before scaling.\n",
    "- Use Dask for large data that doesn't fit in memory.\n",
    "- Use Swifter/Modin when apply-based logic is unavoidable.\n",
    "\n",
    "⚠️ **Avoid:**\n",
    "- Mixing Pandas and Dask APIs in the same pipeline without care.\n",
    "- Over-partitioning Dask DataFrames (too many small tasks can degrade performance).\n",
    "- Expecting `apply()` to parallelize automatically—it won’t without helper libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Exercise\n",
    "\n",
    "**Task:** Load a 2M-row CSV file of user activity logs and:\n",
    "1. Compute average session duration per user using **vectorized operations**.\n",
    "2. Apply a custom scoring function using **Swifter**.\n",
    "3. Re-implement the pipeline using **Dask** to process lazily and compare execution time.\n",
    "\n",
    "_Hint:_ Use `dd.read_csv()` and `.compute()` for final evaluation.\n",
    "\n",
    "# --- End of Add-on Module Section 2 ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
