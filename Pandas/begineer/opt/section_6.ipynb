{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§© Add-on Module 6: Machine Learning & Data Preprocessing with Pandas\n",
    "\n",
    "Machine learning projects start long before model training â€” data wrangling, cleaning, and feature engineering often take **80%** of your time.\n",
    "\n",
    "This module focuses on using **Pandas** for end-to-end ML data preparation:\n",
    "- Handling missing values and outliers\n",
    "- Encoding categorical variables\n",
    "- Normalizing and scaling features\n",
    "- Creating time-based and interaction features\n",
    "- Integrating Pandas DataFrames with **scikit-learn** pipelines\n",
    "\n",
    "By the end, youâ€™ll be able to transform raw business data into model-ready inputs efficiently. ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Handling Missing Data\n",
    "Missing data can bias or break ML models. Pandas provides multiple methods to inspect and impute missing values."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Simulate raw customer data\n",
    "data = {\n",
    "    'age': [25, 32, np.nan, 40, 29, np.nan, 38],\n",
    "    'income': [50000, 60000, 58000, np.nan, 52000, 49000, np.nan],\n",
    "    'city': ['New York', 'Paris', 'Paris', 'Berlin', 'Berlin', np.nan, 'Tokyo']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation Strategies\n",
    "- **Mean/Median Imputation** for numeric values.\n",
    "- **Mode/Constant Imputation** for categorical data.\n",
    "- **Forward/Backward Fill** for time series."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Fill numeric columns with median\n",
    "df['age'] = df['age'].fillna(df['age'].median())\n",
    "df['income'] = df['income'].fillna(df['income'].mean())\n",
    "\n",
    "# Fill categorical with mode\n",
    "df['city'] = df['city'].fillna(df['city'].mode()[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoding Categorical Variables\n",
    "\n",
    "Machine learning models work on numbers, not strings. Pandas provides several techniques for encoding categorical features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Label Encoding (simple mapping)\n",
    "city_map = {city: i for i, city in enumerate(df['city'].unique())}\n",
    "df['city_encoded'] = df['city'].map(city_map)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding using `pd.get_dummies()`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_encoded = pd.get_dummies(df, columns=['city'], prefix='city')\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Scaling & Normalization\n",
    "Scaling ensures that no single feature dominates due to differing scales. This is essential for algorithms like **KNN**, **SVM**, and **Gradient Descentâ€“based models**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df_encoded[['age', 'income']])\n",
    "\n",
    "df_encoded[['age_scaled', 'income_scaled']] = scaled_features\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Feature engineering helps extract hidden patterns from raw data. Letâ€™s demonstrate a few useful transformations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Interaction feature\n",
    "df_encoded['income_per_age'] = df_encoded['income'] / df_encoded['age']\n",
    "\n",
    "# Binning continuous variables\n",
    "df_encoded['age_group'] = pd.cut(df_encoded['age'], bins=[0, 25, 35, 45, 60], labels=['<25', '25-35', '35-45', '45+'])\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date-Time Features Example"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sales = pd.DataFrame({\n",
    "    'date': pd.date_range(start='2023-01-01', periods=10, freq='D'),\n",
    "    'revenue': np.random.randint(100, 1000, 10)\n",
    "})\n",
    "\n",
    "sales['day'] = sales['date'].dt.day\n",
    "sales['weekday'] = sales['date'].dt.day_name()\n",
    "sales['is_weekend'] = sales['weekday'].isin(['Saturday', 'Sunday'])\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Integrating Pandas with scikit-learn Pipelines\n",
    "\n",
    "Pandas works seamlessly with scikit-learn for ML preprocessing and modeling.\n",
    "Letâ€™s build a quick ML-ready preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "numeric_features = ['age', 'income']\n",
    "categorical_features = ['city']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', LinearRegression())])\n",
    "\n",
    "# Mock target variable\n",
    "y = np.random.randint(2000, 6000, len(df))\n",
    "\n",
    "# Train pipeline\n",
    "model.fit(df, y)\n",
    "pred = model.predict(df)\n",
    "pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real-World Problem 1: Predict Employee Salaries\n",
    "\n",
    "**Scenario:** You have employee demographic data (age, role, experience, city) and want to predict expected salary.\n",
    "\n",
    "**Steps:**\n",
    "1. Clean missing experience and city fields.\n",
    "2. Encode roles and cities with One-Hot encoding.\n",
    "3. Normalize numeric columns.\n",
    "4. Train a Linear Regression model.\n",
    "\n",
    "ðŸ’¡ Use Pandas pipelines and `ColumnTransformer` as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-World Problem 2: Customer Churn Prediction\n",
    "\n",
    "**Scenario:** Telecom company dataset with columns â€” age, plan, monthly_charge, usage_minutes, churn (target)\n",
    "\n",
    "**Goal:** Build ML-ready data with Pandas:\n",
    "- Handle missing usage data.\n",
    "- Encode plan types.\n",
    "- Derive new features like charge per minute.\n",
    "- Output final `X_train`, `y_train` DataFrames ready for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Under the Hood\n",
    "\n",
    "- Pandasâ€™ internal block manager efficiently stores mixed dtypes.\n",
    "- `DataFrame.values` â†’ NumPy arrays, directly used by scikit-learn.\n",
    "- Transformations like `get_dummies()` and `merge()` preserve alignment by index.\n",
    "- scikit-learnâ€™s `ColumnTransformer` wraps these transformations for reproducible pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Best Practices / Pitfalls\n",
    "\n",
    "**Best Practices:**\n",
    "- Always use `SimpleImputer` and `ColumnTransformer` for consistent preprocessing.\n",
    "- Prefer **vectorized** feature engineering over loops.\n",
    "- Keep preprocessing pipelines reproducible using **sklearn Pipelines**.\n",
    "\n",
    "**Pitfalls:**\n",
    "- Be cautious when applying `get_dummies()` on unseen test categories.\n",
    "- Avoid scaling categorical columns.\n",
    "- Donâ€™t mix fitted transformers across train/test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’ª Challenge Exercise\n",
    "\n",
    "**Task:**\n",
    "1. Load any public dataset (e.g., Titanic, Housing).\n",
    "2. Handle missing values using Pandas.\n",
    "3. Engineer new features (ratios, interactions, flags).\n",
    "4. Build a scikit-learn pipeline to train a classification model.\n",
    "5. Compare model performance with and without feature scaling.\n",
    "\n",
    "_Hint:_ Try experimenting with different encoding schemes (`pd.factorize`, `OneHotEncoder`, `OrdinalEncoder`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
