{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Add-on Module 5: Integrating Pandas with Databases, APIs & Big Data Systems\n",
    "\n",
    "This module covers how to move Pandas beyond in-memory data processing ‚Äî connecting it to **SQL databases, REST APIs, and distributed formats** for scalable data engineering.\n",
    "\n",
    "By the end, you will learn to:\n",
    "- Read/write efficiently to **SQL databases**.\n",
    "- Fetch and process live data from **REST APIs**.\n",
    "- Work with **Parquet**, **Feather**, and **Arrow** formats for large-scale data.\n",
    "- Integrate **DuckDB** and **BigQuery** for analytics at scale.\n",
    "\n",
    "Let's dive in! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Working with SQL Databases\n",
    "\n",
    "Pandas integrates tightly with SQLAlchemy, allowing direct I/O with databases like MySQL, PostgreSQL, SQLite, etc."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create a SQLite database (for demo)\n",
    "engine = create_engine('sqlite:///sales.db', echo=False)\n",
    "\n",
    "# Create sample data\n",
    "sales = pd.DataFrame({\n",
    "    'product': ['A', 'B', 'C', 'D'],\n",
    "    'region': ['North', 'East', 'West', 'South'],\n",
    "    'revenue': [10000, 15000, 12000, 17000]\n",
    "})\n",
    "\n",
    "# Write to SQL\n",
    "sales.to_sql('sales_data', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# Read from SQL\n",
    "df_sql = pd.read_sql('SELECT * FROM sales_data', con=engine)\n",
    "df_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Tip:** For large datasets, use `chunksize` in `read_sql` to stream results efficiently.\n",
    "\n",
    "```python\n",
    "for chunk in pd.read_sql('SELECT * FROM big_table', con=engine, chunksize=10000):\n",
    "    process(chunk)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Interacting with REST APIs\n",
    "\n",
    "APIs provide real-time data feeds. You can use `requests` to fetch JSON and easily convert it into Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "# Example: Fetch cryptocurrency market data from CoinDesk API\n",
    "url = 'https://api.coindesk.com/v1/bpi/currentprice.json'\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_api = pd.DataFrame.from_dict(data['bpi'], orient='index')\n",
    "df_api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-World Example: Weather Forecast API\n",
    "You can use public weather APIs like Open-Meteo to fetch daily weather summaries.\n",
    "\n",
    "```python\n",
    "url = 'https://api.open-meteo.com/v1/forecast?latitude=52.52&longitude=13.41&daily=temperature_2m_max&timezone=auto'\n",
    "data = requests.get(url).json()\n",
    "df_weather = pd.DataFrame({\n",
    "    'date': data['daily']['time'],\n",
    "    'max_temp': data['daily']['temperature_2m_max']\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Working with Parquet, Feather, and Arrow\n",
    "\n",
    "Columnar formats like Parquet and Feather are optimized for performance and interoperability with Spark, Arrow, and Dask."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = pd.DataFrame({\n",
    "    'user_id': range(1, 6),\n",
    "    'score': [85, 90, 76, 88, 92]\n",
    "})\n",
    "\n",
    "# Save as Parquet\n",
    "df.to_parquet('scores.parquet')\n",
    "\n",
    "# Load back\n",
    "df_parquet = pd.read_parquet('scores.parquet')\n",
    "df_parquet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Performance Comparison:**\n",
    "| Format | Compression | Speed | Use Case |\n",
    "|---------|--------------|--------|-----------|\n",
    "| CSV | None | Slow | Simple exchange |\n",
    "| Parquet | Yes | Fast | Analytics, Big Data |\n",
    "| Feather | Optional | Very Fast | In-memory pipeline |\n",
    "| Arrow | Yes | Very Fast | Cross-language data sharing |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DuckDB Integration\n",
    "\n",
    "DuckDB is an **in-process analytical database** that works natively with Pandas and Parquet ‚Äî great for query-heavy workloads."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install -q duckdb\n",
    "import duckdb\n",
    "\n",
    "# Query Pandas DataFrame directly using DuckDB\n",
    "query_result = duckdb.query('SELECT AVG(revenue) AS avg_rev FROM sales').to_df()\n",
    "query_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Advantages:**\n",
    "- No external server ‚Äî runs locally.\n",
    "- Queries large Parquet files directly.\n",
    "- Integrates with Pandas, Arrow, and Polars seamlessly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Integration with BigQuery / Cloud Data Warehouses\n",
    "\n",
    "Pandas can push queries and pull results from BigQuery, Snowflake, or Redshift using their official Python clients."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example (Google BigQuery)\n",
    "'''\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "query = 'SELECT name, SUM(number) as total FROM `bigquery-public-data.usa_names.usa_1910_2013` GROUP BY name LIMIT 5'\n",
    "df_bq = client.query(query).to_dataframe()\n",
    "df_bq.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real-World Problems\n",
    "\n",
    "### Problem 1: Build a Data Pipeline from API to Database\n",
    "**Goal:** Fetch daily COVID-19 case data from an API, clean it with Pandas, and store it in SQLite.\n",
    "\n",
    "Steps:\n",
    "1. Request JSON data using `requests`.\n",
    "2. Convert into a normalized Pandas DataFrame.\n",
    "3. Save to SQL using `.to_sql()`.\n",
    "4. Visualize top-5 countries by case count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Querying and Aggregating Parquet Data\n",
    "**Goal:** Analyze millions of sales records stored in Parquet files using DuckDB and Pandas.\n",
    "\n",
    "Steps:\n",
    "1. Use DuckDB to query aggregated data directly from Parquet.\n",
    "2. Convert query results into Pandas.\n",
    "3. Plot sales trends using Matplotlib.\n",
    "\n",
    "```python\n",
    "query = \"SELECT region, SUM(revenue) as total_rev FROM 'sales.parquet' GROUP BY region\"\n",
    "df_agg = duckdb.query(query).to_df()\n",
    "df_agg.plot(kind='bar', x='region', y='total_rev', title='Revenue by Region')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices\n",
    "\n",
    "‚úÖ **Best Practices:**\n",
    "- Always use **SQLAlchemy** for flexible DB connections.\n",
    "- Prefer **Parquet** for high-volume analytics.\n",
    "- Cache API responses locally to reduce rate limits.\n",
    "- Offload heavy queries to **DuckDB** or **BigQuery**.\n",
    "\n",
    "‚ö†Ô∏è **Pitfalls:**\n",
    "- Avoid reading entire API payloads if pagination exists.\n",
    "- Don‚Äôt store sensitive credentials in plain text.\n",
    "- Ensure schema consistency between Pandas and SQL types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Challenge Exercise\n",
    "\n",
    "**Task:**\n",
    "1. Pull public stock data for 10 tickers using a REST API.\n",
    "2. Combine and clean the dataset with Pandas.\n",
    "3. Store it in SQLite and query top-3 gainers using SQL.\n",
    "4. Save cleaned dataset to Parquet.\n",
    "\n",
    "_Hint:_ Use `yfinance` or `AlphaVantage` API and explore DuckDB queries over the resulting Parquet files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
