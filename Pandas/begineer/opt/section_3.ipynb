{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Add-on Module 3: Integration of Pandas with Other Libraries\n",
    "\n",
    "In this module, we explore **how Pandas integrates with other data ecosystems** — combining flexibility with performance. We'll cover:\n",
    "\n",
    "- Seamless exchange with **NumPy** arrays\n",
    "- Database operations using **SQLAlchemy**\n",
    "- High-speed interchange with **PyArrow** and **Polars**\n",
    "- Hybrid workflows and conversion performance insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pandas ↔ NumPy Interoperability\n",
    "\n",
    "Since Pandas is built atop NumPy, conversion between DataFrames and NumPy arrays is almost zero-cost."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'x': np.arange(10),\n",
    "    'y': np.random.randn(10)\n",
    "})\n",
    "\n",
    "# Convert to NumPy\n",
    "arr = df.to_numpy()\n",
    "print(arr[:5])\n",
    "\n",
    "# Convert back to DataFrame\n",
    "df_back = pd.DataFrame(arr, columns=['x', 'y'])\n",
    "df_back.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under the Hood:\n",
    "- `DataFrame.to_numpy()` provides a **view** where possible, not a copy.\n",
    "- Numeric columns share memory with the original DataFrame.\n",
    "- `df.values` (legacy) and `.to_numpy()` differ subtly — the latter respects dtype consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pandas ↔ SQL Integration (Using SQLAlchemy)\n",
    "\n",
    "Pandas can read and write directly to SQL databases, enabling hybrid pipelines between relational storage and in-memory analysis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create in-memory SQLite database\n",
    "engine = create_engine('sqlite://', echo=False)\n",
    "\n",
    "# Write DataFrame to SQL\n",
    "df.to_sql('metrics', con=engine, index=False, if_exists='replace')\n",
    "\n",
    "# Query back into Pandas\n",
    "result_df = pd.read_sql('SELECT * FROM metrics WHERE x < 5', con=engine)\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Real-World Problem 1: Data Warehouse Integration**\n",
    "\n",
    "**Scenario:** You collect transactional data into PostgreSQL but need in-memory analytics.\n",
    "\n",
    "```python\n",
    "engine = create_engine('postgresql+psycopg2://user:password@localhost:5432/salesdb')\n",
    "orders = pd.read_sql('SELECT * FROM orders WHERE date >= CURRENT_DATE - INTERVAL \\'30 days\\'', engine)\n",
    "orders.groupby('region')['revenue'].sum().sort_values(ascending=False).head()\n",
    "```\n",
    "\n",
    "This approach combines SQL’s filtering power with Pandas’ analytical agility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pandas ↔ PyArrow Interchange\n",
    "\n",
    "**PyArrow** enables efficient columnar data exchange and zero-copy interoperability between Pandas, Spark, and other big data systems."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "# Convert Pandas DataFrame to Arrow Table\n",
    "table = pa.Table.from_pandas(df)\n",
    "print(table.schema)\n",
    "\n",
    "# Convert back to Pandas\n",
    "df_arrow = table.to_pandas()\n",
    "df_arrow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-World Problem 2: Arrow-based Data Exchange\n",
    "\n",
    "**Scenario:** You want to share preprocessed datasets with a Spark or DuckDB workflow.\n",
    "\n",
    "```python\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Save to Parquet format\n",
    "pq.write_table(table, 'data/output.parquet')\n",
    "\n",
    "# Read back efficiently\n",
    "table_loaded = pq.read_table('data/output.parquet')\n",
    "df_loaded = table_loaded.to_pandas()\n",
    "```\n",
    "\n",
    "✅ Arrow enables fast binary exchange, ideal for multi-language pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pandas ↔ Polars Integration\n",
    "\n",
    "**Polars** is a Rust-based DataFrame library offering lightning-fast operations using Apache Arrow memory format. You can easily interconvert with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install -q polars\n",
    "import polars as pl\n",
    "\n",
    "# Convert Pandas → Polars\n",
    "pl_df = pl.from_pandas(df)\n",
    "\n",
    "# Perform fast lazy computations\n",
    "lazy_df = pl_df.lazy().filter(pl.col('x') > 3).select(['x', 'y'])\n",
    "result = lazy_df.collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under the Hood:\n",
    "- **Polars** uses Arrow buffers, supporting zero-copy conversion.\n",
    "- Its lazy engine builds query plans, optimizing filters and projections.\n",
    "- Great for large ETL pipelines and hybrid workflows with Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hybrid Workflow Example: From SQL → Pandas → Polars → Arrow\n",
    "\n",
    "An end-to-end example combining all integration points."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example hybrid pipeline\n",
    "df_sql = pd.read_sql('SELECT * FROM metrics', con=engine)\n",
    "pl_df = pl.from_pandas(df_sql)\n",
    "arrow_tbl = pa.Table.from_pandas(pl_df.to_pandas())\n",
    "pq.write_table(arrow_tbl, 'metrics_final.parquet')\n",
    "\n",
    "print('Pipeline completed and saved as Parquet file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices / Pitfalls\n",
    "\n",
    "✅ **Best Practices:**\n",
    "- Use Arrow for cross-language or distributed data interchange.\n",
    "- Keep schema consistent when switching between engines.\n",
    "- Prefer SQLAlchemy over raw connectors for database operations.\n",
    "- Benchmark conversions when chaining multiple backends.\n",
    "\n",
    "⚠️ **Pitfalls:**\n",
    "- Converting between formats repeatedly can increase memory overhead.\n",
    "- Some dtypes (like complex objects) aren’t natively supported by Arrow/Polars.\n",
    "- Avoid mixing lazy and eager evaluations without explicit `.collect()` or `.compute()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Exercise\n",
    "\n",
    "**Task:** Build a hybrid ETL pipeline that:**\n",
    "1. Reads data from a SQL database into Pandas.\n",
    "2. Converts it to Polars for transformations.\n",
    "3. Saves the final result as a Parquet file via PyArrow.\n",
    "\n",
    "_Bonus_: Benchmark the total runtime compared to a pure Pandas workflow.\n",
    "\n",
    "# --- End of Add-on Module Section 3 ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
