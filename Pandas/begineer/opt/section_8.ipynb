{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß© Add-on Module 8: Profiling, Optimization & Best Practices for Production Pandas\n",
    "\n",
    "In this module, we explore how to move from analysis to **production-quality Pandas pipelines**:\n",
    "\n",
    "- Profiling slow and memory-heavy code\n",
    "- Understanding the internal execution model\n",
    "- Applying vectorization and parallelization\n",
    "- Using efficient data formats\n",
    "- Establishing reproducible, scalable patterns\n",
    "\n",
    "We'll apply these lessons to a **real-world ETL (Extract, Transform, Load)** scenario with millions of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Profiling Pandas Performance\n",
    "\n",
    "Before optimizing, you must identify the bottlenecks.\n",
    "\n",
    "### Tools for Profiling:\n",
    "- `%timeit` and `%prun` in IPython/Jupyter\n",
    "- `cProfile` for call-level profiling\n",
    "- `memory_profiler` for RAM usage\n",
    "- `line_profiler` for per-line CPU cost"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data\n",
    "n = 2_000_000\n",
    "df = pd.DataFrame({\n",
    "    'user_id': np.random.randint(1, 50000, n),\n",
    "    'amount': np.random.uniform(10, 1000, n),\n",
    "    'category': np.random.choice(['electronics', 'grocery', 'fashion', 'home'], n)\n",
    "})\n",
    "\n",
    "# Example: Profiling aggregation time\n",
    "%timeit df.groupby('category')['amount'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Memory Profiling Example"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from memory_profiler import memory_usage\n",
    "\n",
    "def summarize():\n",
    "    return df.groupby('category')['amount'].mean()\n",
    "\n",
    "mem_usage = memory_usage(summarize)\n",
    "print(f\"Memory used: {max(mem_usage) - min(mem_usage):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Vectorization vs Loops\n",
    "\n",
    "The biggest performance killer in Pandas is **Python-level loops**.\n",
    "\n",
    "Use vectorized operations (powered by NumPy‚Äôs C-level speed) instead."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ‚ùå Bad: Python loop\n",
    "def loop_sum(df):\n",
    "    total = []\n",
    "    for amt in df['amount']:\n",
    "        total.append(amt * 1.18)\n",
    "    df['taxed'] = total\n",
    "    return df\n",
    "\n",
    "# ‚úÖ Good: Vectorized operation\n",
    "df['taxed'] = df['amount'] * 1.18\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Using Efficient Data Types\n",
    "\n",
    "Reducing data types can drastically shrink memory usage."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.info(memory_usage='deep')\n",
    "\n",
    "# Convert types for optimization\n",
    "df['user_id'] = df['user_id'].astype('int32')\n",
    "df['amount'] = df['amount'].astype('float32')\n",
    "df['category'] = df['category'].astype('category')\n",
    "\n",
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ I/O Optimization: Parquet & Feather\n",
    "\n",
    "When dealing with large files, the storage format matters.\n",
    "\n",
    "**CSV** is human-readable but slow. Prefer **binary formats** like:\n",
    "\n",
    "- `.parquet` (columnar, compressed, great for analytics)\n",
    "- `.feather` (lightweight and fast)\n",
    "\n",
    "These formats support **predicate pushdown** and are ideal for incremental ETL."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save as Parquet\n",
    "df.to_parquet('optimized_sales.parquet', index=False)\n",
    "\n",
    "# Reload faster than CSV\n",
    "df2 = pd.read_parquet('optimized_sales.parquet')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Caching and Chunk Processing\n",
    "\n",
    "When data is too big to fit into RAM, process it in chunks.\n",
    "\n",
    "You can combine chunking with `HDF5`, `SQLite`, or Dask for streaming-style processing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "chunk_iter = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv', chunksize=20)\n",
    "summary = []\n",
    "for chunk in chunk_iter:\n",
    "    summary.append(chunk['tip'].mean())\n",
    "\n",
    "print(f\"Average Tip (chunked processing): {np.mean(summary):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Real-World Problem 1: Financial Transaction Pipeline\n",
    "\n",
    "**Scenario:**\n",
    "A fintech company receives millions of transactions daily. You need to:\n",
    "\n",
    "- Detect abnormal spending patterns\n",
    "- Optimize ETL time from 4 hours ‚Üí under 30 minutes\n",
    "- Reduce dataset memory by 80%\n",
    "\n",
    "**Approach:**\n",
    "1. Use Parquet instead of CSV\n",
    "2. Convert strings to categories\n",
    "3. Apply vectorized operations for fraud scoring\n",
    "4. Use Dask for lazy parallel loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Real-World Problem 2: Marketing Data Cleanup\n",
    "\n",
    "**Scenario:**\n",
    "Marketing data includes millions of customer events (clicks, purchases, ad views).\n",
    "\n",
    "**Goal:** Deduplicate by (user_id, event_time), compress, and compute daily metrics efficiently.\n",
    "\n",
    "**Approach:**\n",
    "- Use `df.drop_duplicates(['user_id', 'event_time'], keep='last')`\n",
    "- Convert timestamps to `datetime64[ns]`\n",
    "- Cache daily summaries to Parquet\n",
    "- Use memory-profiler to ensure process stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Under the Hood\n",
    "\n",
    "- **GroupBy & Aggregations** use hash tables internally.\n",
    "- **Categorical columns** store integer codes + dictionary mapping.\n",
    "- **Vectorized math** uses NumPy‚Äôs C/Fortran-level loops.\n",
    "- **I/O acceleration** relies on Apache Arrow & PyArrow libraries.\n",
    "- **Lazy evaluation (in Dask/Polars)** builds computation graphs before executing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Best Practices Checklist\n",
    "\n",
    "- [x] Use `df.info(memory_usage='deep')` to audit RAM\n",
    "- [x] Prefer `.parquet` or `.feather` for I/O\n",
    "- [x] Convert categorical and integer types\n",
    "- [x] Avoid `apply()` loops when possible\n",
    "- [x] Cache intermediate results in local disk or Arrow buffers\n",
    "- [x] Profile before optimizing ‚Äî don‚Äôt guess!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Challenge Exercise\n",
    "\n",
    "You have a 3GB CSV of user analytics logs.\n",
    "\n",
    "1. Profile load time and memory usage.\n",
    "2. Reduce memory by 70% using dtype conversion.\n",
    "3. Save it as Parquet and compare I/O time.\n",
    "4. Implement a Dask-based version for incremental processing.\n",
    "5. Visualize the top 10 users with the most events per day."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
