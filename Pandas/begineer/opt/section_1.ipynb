{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß© Add-on Module 1: Performance Optimization & Memory Efficiency\n",
    "**Level:** Advanced\n",
    "\n",
    "---\n",
    "## üéØ Learning Objectives\n",
    "In this module, you will:\n",
    "- Understand how Pandas‚Äô internal data structures impact performance  \n",
    "- Learn to **profile**, **optimize**, and **accelerate** DataFrame operations  \n",
    "- Explore **vectorization**, **categorical encoding**, and **in-place updates**  \n",
    "- Compare performance across optimization strategies  \n",
    "- Apply these techniques to a **real-world retail dataset (~5M rows)**  \n",
    "\n",
    "---\n",
    "## üß† Why Optimization Matters\n",
    "Pandas is incredibly powerful but also **memory-bound** and **single-threaded**.\n",
    "Without optimization, operations on large DataFrames can become slow or even crash due to RAM exhaustion.\n",
    "\n",
    "With efficient use of **data types**, **vectorization**, and **in-place updates**, you can often make pipelines **10x faster** and **10x smaller in memory footprint**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ 1.1 Measuring Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a large synthetic dataset\n",
    "N = 1_000_000\n",
    "df = pd.DataFrame({\n",
    "    'user_id': np.random.randint(1, 100_000, size=N),\n",
    "    'age': np.random.randint(18, 70, size=N),\n",
    "    'city': np.random.choice(['New York', 'Paris', 'Berlin', 'Tokyo', 'Delhi'], size=N),\n",
    "    'spend': np.random.uniform(10.0, 1000.0, size=N)\n",
    "})\n",
    "\n",
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° `memory_usage='deep'` provides a full estimate including Python objects like strings.\n",
    "Next, we‚Äôll optimize these columns for better memory efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ 1.2 Optimizing Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'city' to category\n",
    "df['city'] = df['city'].astype('category')\n",
    "\n",
    "# Downcast numeric columns\n",
    "df['user_id'] = pd.to_numeric(df['user_id'], downcast='unsigned')\n",
    "df['age'] = pd.to_numeric(df['age'], downcast='unsigned')\n",
    "df['spend'] = pd.to_numeric(df['spend'], downcast='float')\n",
    "\n",
    "# Compare memory usage\n",
    "optimized_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f'Optimized Memory Usage: {optimized_memory:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Best Practices\n",
    "- Convert `object` columns to `category` when there are repeated strings.\n",
    "- Use `downcast` to choose the smallest numeric dtype that fits the data.\n",
    "- Avoid `float64` unless high precision is essential.\n",
    "- Store timestamps in `datetime64[ns]` for efficient arithmetic and filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ 1.3 Vectorization vs. Loops\n",
    "\n",
    "Pandas is built on top of **NumPy**, so vectorized operations are much faster than Python loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def loop_method(df):\n",
    "    result = []\n",
    "    for s in df['spend']:\n",
    "        result.append(s * 1.05)\n",
    "    df['spend_taxed_loop'] = result\n",
    "\n",
    "def vectorized_method(df):\n",
    "    df['spend_taxed_vec'] = df['spend'] * 1.05\n",
    "\n",
    "# Benchmark\n",
    "start = time.time()\n",
    "loop_method(df.copy())\n",
    "print(f'Loop Time: {time.time() - start:.4f}s')\n",
    "\n",
    "start = time.time()\n",
    "vectorized_method(df.copy())\n",
    "print(f'Vectorized Time: {time.time() - start:.4f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß© **Result:** Vectorized operations can be **50‚Äì200x faster** than loops, since they use NumPy‚Äôs C-level backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ 1.4 Using `eval()` and `query()` for Faster Computations\n",
    "\n",
    "Pandas provides `eval()` and `query()` for compiling and executing expressions in C, improving performance and reducing memory overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.DataFrame({\n",
    "    'price': np.random.uniform(5, 500, size=1_000_000),\n",
    "    'quantity': np.random.randint(1, 10, size=1_000_000)\n",
    "})\n",
    "\n",
    "# Regular computation\n",
    "%timeit sales['total'] = sales['price'] * sales['quantity']\n",
    "\n",
    "# Using eval()\n",
    "%timeit sales.eval('total = price * quantity', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚öôÔ∏è `eval()` and `query()` are best used for:\n",
    "- Large DataFrames with repetitive arithmetic\n",
    "- Expressions with multiple columns\n",
    "- Cases where temporary DataFrames are expensive to create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ 1.5 Real-world Case Study: Retail Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 5_000_000\n",
    "retail = pd.DataFrame({\n",
    "    'transaction_id': np.arange(rows),\n",
    "    'customer_id': np.random.randint(1, 500_000, rows),\n",
    "    'country': np.random.choice(['US', 'UK', 'DE', 'IN', 'AU'], rows),\n",
    "    'amount': np.random.uniform(10, 1000, rows),\n",
    "    'tax_rate': np.random.uniform(0.05, 0.18, rows)\n",
    "})\n",
    "\n",
    "print(f'Memory Before: {retail.memory_usage(deep=True).sum() / 1024**2:.2f} MB')\n",
    "\n",
    "# Optimize dtypes\n",
    "retail['country'] = retail['country'].astype('category')\n",
    "retail['customer_id'] = pd.to_numeric(retail['customer_id'], downcast='unsigned')\n",
    "retail['amount'] = pd.to_numeric(retail['amount'], downcast='float')\n",
    "retail['tax_rate'] = pd.to_numeric(retail['tax_rate'], downcast='float')\n",
    "\n",
    "print(f'Memory After: {retail.memory_usage(deep=True).sum() / 1024**2:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Memory usage can often be reduced by **3‚Äì4x** simply through categorical encoding and numeric downcasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ 1.6 Profiling and Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "start = perf_counter()\n",
    "retail.eval('total = amount + (amount * tax_rate)', inplace=True)\n",
    "end = perf_counter()\n",
    "\n",
    "print(f'Execution Time: {end - start:.3f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß© Challenge: Optimize a Customer Dataset\n",
    "\n",
    "You are given a CSV file with the following columns:\n",
    "`customer_id`, `gender`, `region`, `income`, `purchases`\n",
    "\n",
    "Tasks:\n",
    "1. Load and inspect memory usage.  \n",
    "2. Convert optimal data types (`region ‚Üí category`, `income ‚Üí float32`).  \n",
    "3. Compare runtime of computing average income using:\n",
    "   - A loop  \n",
    "   - Vectorized `groupby()`  \n",
    "4. Report memory and performance improvements.\n",
    "\n",
    "üí° Hint: Use `df.memory_usage(deep=True)` and `%timeit` for benchmarking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìò Summary\n",
    "- ‚úÖ Use **categories** and **downcasting** for memory efficiency.  \n",
    "- ‚úÖ Avoid Python loops ‚Äî prefer **vectorized** and **eval()** operations.  \n",
    "- ‚úÖ Profile before optimizing ‚Äî use `%timeit`, `perf_counter()`, and `df.info()`.  \n",
    "- ‚úÖ Optimization = more rows processed, less RAM used, faster pipelines.\n",
    "\n",
    "---\n",
    "### üöÄ Next Module ‚Üí Parallelization & Scaling with Dask\n",
    "Learn how to scale Pandas computations across multiple CPU cores or even clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
