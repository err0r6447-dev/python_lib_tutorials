{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Section 10: Performance Optimization & Integration with Other Libraries\n",
    "\n",
    "**Level:** Advanced / Add-on Module\n",
    "\n",
    "Pandas is highly capable but can become slow when dealing with very large datasets. This section explores techniques to **optimize performance**, **profile bottlenecks**, and **integrate Pandas with other high-performance libraries** like NumPy, Polars, and SQLAlchemy.\n",
    "\n",
    "We'll cover:\n",
    "- Profiling Pandas performance\n",
    "- Vectorization & avoiding Python loops\n",
    "- Memory optimization techniques\n",
    "- Parallel processing with Dask\n",
    "- Integration with NumPy, Polars, and databases\n",
    "- Real-world scalability examples\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ 10.1 Profiling Pandas Performance\n",
    "\n",
    "Use the `%%timeit` Jupyter magic or the `time` module to identify slow operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Create a large dataset\n",
    "N = 1_000_000\n",
    "df = pd.DataFrame({\n",
    "    'A': np.random.randint(0, 100, size=N),\n",
    "    'B': np.random.randn(N),\n",
    "    'C': np.random.choice(['X', 'Y', 'Z'], size=N)\n",
    "})\n",
    "\n",
    "# Time inefficient vs efficient operations\n",
    "start = time.time()\n",
    "df['A_squared_loop'] = [x**2 for x in df['A']]\n",
    "print('Loop time:', round(time.time() - start, 3), 's')\n",
    "\n",
    "start = time.time()\n",
    "df['A_squared_vec'] = df['A']**2\n",
    "print('Vectorized time:', round(time.time() - start, 3), 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Takeaway:** Always prefer **vectorized NumPy-style operations** over Python loops for scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ 10.2 Memory Optimization\n",
    "\n",
    "Reducing memory footprint is critical when working with large DataFrames. Pandas offers utilities to **downcast numeric types** and **convert object columns to categories**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df.copy()\n",
    "\n",
    "# Convert integer columns to smaller dtype\n",
    "df_small['A'] = pd.to_numeric(df_small['A'], downcast='unsigned')\n",
    "\n",
    "# Convert float columns\n",
    "df_small['B'] = pd.to_numeric(df_small['B'], downcast='float')\n",
    "\n",
    "# Convert categorical columns\n",
    "df_small['C'] = df_small['C'].astype('category')\n",
    "\n",
    "print('Memory usage before:', round(df.memory_usage(deep=True).sum() / 1e6, 2), 'MB')\n",
    "print('Memory usage after:', round(df_small.memory_usage(deep=True).sum() / 1e6, 2), 'MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ 10.3 Parallel and Lazy Computation with Dask\n",
    "\n",
    "Dask extends Pandas for **out-of-core** (too-large-for-memory) and **parallel** processing. It uses the same API, making it easy to scale up existing Pandas workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Convert Pandas DataFrame to Dask DataFrame\n",
    "dask_df = dd.from_pandas(df, npartitions=8)\n",
    "\n",
    "# Perform parallel groupby computation\n",
    "result = dask_df.groupby('C')['A'].mean().compute()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Takeaway:** Dask is ideal for large datasets or multi-core machines ‚Äî it parallelizes operations transparently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ 10.4 Integration with NumPy\n",
    "\n",
    "Pandas is built on top of NumPy ‚Äî meaning all numerical computations ultimately delegate to efficient NumPy arrays.\n",
    "\n",
    "You can access NumPy arrays directly via `.values` or `.to_numpy()` for performance-critical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: fast numerical computation using NumPy\n",
    "A_np = df['A'].to_numpy()\n",
    "B_np = df['B'].to_numpy()\n",
    "\n",
    "# Compute correlation using NumPy\n",
    "correlation = np.corrcoef(A_np, B_np)[0, 1]\n",
    "correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ 10.5 Integration with Polars for Speed\n",
    "\n",
    "[Polars](https://pola.rs) is a high-performance DataFrame library written in Rust. It is **multi-threaded** and **lazy-evaluated**, often outperforming Pandas by 5‚Äì10x for certain workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Convert from Pandas to Polars\n",
    "pl_df = pl.from_pandas(df)\n",
    "\n",
    "# Fast groupby operation\n",
    "pl_result = pl_df.groupby('C').agg([\n",
    "    pl.col('A').mean().alias('avg_A'),\n",
    "    pl.col('B').max().alias('max_B')\n",
    "])\n",
    "pl_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Takeaway:** Polars is a great alternative for performance-heavy workloads, especially when handling millions of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ 10.6 Interacting with Databases via SQLAlchemy\n",
    "\n",
    "You can load or write large datasets directly from/to databases using `pandas.read_sql()` and `DataFrame.to_sql()` with **SQLAlchemy** for efficient I/O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# In-memory SQLite database\n",
    "engine = create_engine('sqlite://', echo=False)\n",
    "\n",
    "# Write to SQL\n",
    "df.head(1000).to_sql('sales_data', con=engine, index=False, if_exists='replace')\n",
    "\n",
    "# Query from SQL\n",
    "query_df = pd.read_sql('SELECT C, AVG(A) as avg_A, SUM(B) as sum_B FROM sales_data GROUP BY C', con=engine)\n",
    "query_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Under the Hood\n",
    "\n",
    "- Pandas delegates numeric operations to **NumPy C-level ufuncs**.\n",
    "- Dask and Polars utilize **multi-threading** and **lazy evaluation** for performance.\n",
    "- SQLAlchemy provides an **ORM abstraction** over various database engines.\n",
    "- Memory optimization relies on **bit width reduction** and **categorical encoding**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíº Real-World Problem 1 ‚Äî Large Dataset Aggregation Pipeline\n",
    "\n",
    "**Scenario:** You‚Äôre analyzing 20 million sales records for a retail company. You need to calculate monthly statistics without crashing your machine.\n",
    "\n",
    "**Goal:**\n",
    "1. Use Dask for lazy computation.\n",
    "2. Compute total sales and average discount per month.\n",
    "3. Export the final results to a database for reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Simulate large CSV (use smaller data here for demo)\n",
    "sales = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=10000, freq='H'),\n",
    "    'sales': np.random.randint(100, 1000, 10000),\n",
    "    'discount': np.random.uniform(0.05, 0.3, 10000)\n",
    "})\n",
    "\n",
    "dask_sales = dd.from_pandas(sales, npartitions=8)\n",
    "\n",
    "# Compute monthly metrics\n",
    "monthly_summary = (\n",
    "    dask_sales.assign(month=dask_sales['date'].dt.to_period('M'))\n",
    "    .groupby('month')\n",
    "    .agg({'sales': 'sum', 'discount': 'mean'})\n",
    "    .compute()\n",
    ")\n",
    "monthly_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç Real-World Problem 2 ‚Äî Hybrid Workflow with Polars and Pandas\n",
    "\n",
    "**Scenario:** You receive a 2GB CSV file. You need to preprocess it using Polars for speed, then switch to Pandas for visualization and modeling.\n",
    "\n",
    "**Goal:** Demonstrate an efficient hybrid workflow combining both libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV using Polars (fast)\n",
    "pl_data = pl.DataFrame({\n",
    "    'product': np.random.choice(['A', 'B', 'C'], 10000),\n",
    "    'revenue': np.random.randint(100, 1000, 10000)\n",
    "})\n",
    "\n",
    "# Aggregate in Polars\n",
    "agg = pl_data.groupby('product').agg(pl.col('revenue').mean().alias('avg_revenue'))\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "pd_data = agg.to_pandas()\n",
    "pd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Best Practices / Pitfalls\n",
    "\n",
    "‚úÖ Use vectorized operations instead of loops.\n",
    "‚úÖ Downcast numeric and categorical columns to reduce memory.\n",
    "‚úÖ Use Dask or Polars for large data workloads.\n",
    "‚ö†Ô∏è Avoid chaining many temporary DataFrames ‚Äî use in-place or pipe().\n",
    "‚öôÔ∏è Profile before optimizing ‚Äî premature optimization often backfires.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí™ Challenge Exercise\n",
    "\n",
    "**Task:** You are given a large dataset containing millions of e-commerce transactions.\n",
    "1. Profile its performance bottlenecks.\n",
    "2. Optimize data types to reduce memory usage by 50%.\n",
    "3. Use Dask to compute monthly user purchase totals.\n",
    "4. Integrate results into a SQLite database using `to_sql()`.\n",
    "\n",
    "_Try implementing this full optimization pipeline on your own._\n",
    "\n",
    "---\n",
    "# --- End of Section 10 (Final Add-on Module) ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
