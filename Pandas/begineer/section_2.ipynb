{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 2 — Importing and Exporting Data\n",
        "\n",
        "**Goal:** Learn how to import and export real-world data in Pandas efficiently. We'll explore common file types like CSV, Excel, JSON, and SQL. We'll also learn how to handle missing values and encoding issues during import, and how to serialize clean data back for downstream use.\n",
        "\n",
        "We continue using the *retail sales dataset* from Section 1, but this time simulate reading/writing to disk so code is self-contained and reproducible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Subtopics\n",
        "1. Reading and writing CSV, Excel, and JSON files.\n",
        "2. Managing file encoding, delimiters, and missing data on import.\n",
        "3. Inspecting imported data quickly (`info`, `head`, `nunique`).\n",
        "4. Exporting clean data with appropriate compression and format.\n",
        "5. Handling large files with partial loading (via `chunksize`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "\n",
        "# Create mock CSV data as string (simulate file I/O for notebook use)\n",
        "csv_data = StringIO('''order_id,customer_id,product,quantity,price,order_date\\n'\n",
        "1001,201,T-shirt,2,15.50,2023-01-15\\n'\n",
        "1002,202,Mug,1,9.99,2023-01-22\\n'\n",
        "1003,203,Notebook,3,,2023-01-29\\n'\n",
        "1004,202,Cap,1,12.00,2023-02-05\\n'\n",
        "1005,201,T-shirt,4,16.50,2023-02-12''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reading data from CSV\n",
        "\n",
        "`pd.read_csv()` is the most common import function. It can infer dtypes, parse dates, and handle missing values automatically. We'll use `na_values` and `parse_dates` to illustrate robust reading."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read the CSV into a DataFrame\n",
        "df = pd.read_csv(csv_data, na_values=['', 'NA', 'None'], parse_dates=['order_date'])\n",
        "df.info()\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exporting cleaned data to disk or buffer\n",
        "\n",
        "You can export to CSV, Excel, or JSON formats using `to_csv()`, `to_excel()`, and `to_json()`. Use compression (`.zip`, `.gz`) when saving large files. In practice, always define `index=False` to avoid extra index columns unless explicitly needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Clean data (fill missing price and compute total)\n",
        "df['price'] = df['price'].fillna(df['price'].median())\n",
        "df['total'] = df['price'] * df['quantity']\n",
        "\n",
        "# Export to CSV string buffer for demonstration\n",
        "csv_buffer = StringIO()\n",
        "df.to_csv(csv_buffer, index=False)\n",
        "print(csv_buffer.getvalue().splitlines()[:6])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reading JSON and Excel Files\n",
        "\n",
        "Pandas can natively parse JSON objects and Excel sheets. Here we demonstrate JSON since Excel I/O requires `openpyxl` installed. JSON import is handy for APIs or nested datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convert a small subset to JSON and read it back\n",
        "json_str = df.head(3).to_json(orient='records')\n",
        "print(json_str)\n",
        "df_json = pd.read_json(StringIO(json_str))\n",
        "df_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading data in chunks\n",
        "\n",
        "When datasets are very large, use the `chunksize` parameter to read portions incrementally. Each chunk is a smaller DataFrame that can be processed before moving to the next, avoiding memory exhaustion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example of chunked reading (simulate with a small CSV)\n",
        "csv_data2 = StringIO('\\n'.join([','.join(map(str, row)) for row in df.values]))\n",
        "chunk_iter = pd.read_csv(StringIO(csv_buffer.getvalue()), chunksize=2)\n",
        "\n",
        "totals = []\n",
        "for chunk in chunk_iter:\n",
        "    totals.append(chunk['total'].sum())\n",
        "print('Partial totals per chunk:', totals, '| Grand total:', sum(totals))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real-World Problem 1 — Cleaning a CSV with Mixed Encodings and Missing Values\n",
        "\n",
        "**Scenario:** You receive a sales CSV where some rows use commas while others use semicolons and contain mixed encodings. Demonstrate how to robustly import such data and normalize column names and dtypes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulate a messy CSV using multiple delimiters (normally you'd detect this via sniffing)\n",
        "messy_csv = 'order_id;customer_id;price\\n1001;200;10.5\\n1002,201,12.5\\n1003;202;11.0'\n",
        "\n",
        "# Detect the delimiter (simple heuristic: count semicolons)\n",
        "delimiter = ';' if messy_csv.count(';') > messy_csv.count(',') else ','\n",
        "\n",
        "# Read robustly with encoding fallback\n",
        "df_messy = pd.read_csv(StringIO(messy_csv.replace(';', ',')), encoding_errors='ignore')\n",
        "\n",
        "# Normalize column names\n",
        "df_messy.columns = df_messy.columns.str.strip().str.lower()\n",
        "df_messy.info()\n",
        "df_messy.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real-World Problem 2 — Export a Clean Report for Downstream Analysis\n",
        "\n",
        "**Task:** Starting from our cleaned retail dataset (`df`), export a product-level summary as both JSON and compressed CSV for downstream ML training. Demonstrate format control and compression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Aggregate per product\n",
        "summary = df.groupby('product', as_index=False).agg(\n",
        "    total_qty=('quantity', 'sum'),\n",
        "    total_revenue=('total', 'sum')\n",
        ")\n",
        "\n",
        "# Export to multiple formats\n",
        "json_report = summary.to_json(orient='records', indent=2)\n",
        "csv_buffer2 = StringIO()\n",
        "summary.to_csv(csv_buffer2, index=False, compression='infer')\n",
        "\n",
        "print('JSON Report Example:\\n', json_report[:200], '...')\n",
        "print('\\nCSV Content Preview:\\n', '\\n'.join(csv_buffer2.getvalue().splitlines()[:5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Under the Hood — How Pandas Handles I/O Efficiently\n",
        "\n",
        "- `read_csv()` is implemented in C and optimized with chunked buffering — it can handle millions of rows efficiently.\n",
        "- When `parse_dates` is enabled, Pandas infers datetime columns post-read using vectorized conversion.\n",
        "- Writing functions like `to_csv()` stream data row-by-row; compression is handled by Python’s built-in gzip/bz2 libraries.\n",
        "- When working with Arrow/Parquet backends (in later sections), I/O can become zero-copy, reducing memory footprint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices / Common Pitfalls\n",
        "- Always specify `dtype` or `parse_dates` to prevent unintended `object` dtypes.\n",
        "- Avoid reading massive files into memory at once — use `chunksize`.\n",
        "- Don’t forget `index=False` when writing CSVs unless index carries meaning.\n",
        "- Normalize column names early (lowercase, underscores) for consistency.\n",
        "- For production pipelines, prefer Parquet/Feather over CSV for speed and schema preservation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenge Exercise (no solution here)\n",
        "\n",
        "You’re given a CSV with columns: `customer_id`, `purchase_date`, `amount_spent`, and some missing entries. Write code to:\n",
        "1. Read the CSV robustly (detect delimiter, parse dates).\n",
        "2. Fill missing `amount_spent` with the median per customer.\n",
        "3. Export two versions:\n",
        "   - CSV compressed with gzip.\n",
        "   - JSON formatted with 2-space indentation.\n",
        "4. Verify that the exported JSON file can be reloaded with identical totals per customer.\n",
        "\n",
        "_Hint:_ Compare original vs. re-imported data using `groupby('customer_id').sum()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# --- End of Section 2 — Continue to Section 3 ---"
      ]
    }
  ]
}
