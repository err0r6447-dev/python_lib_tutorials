{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 3 ‚Äî Data Cleaning and Preprocessing\n",
        "\n",
        "### Goal\n",
        "Real-world datasets are rarely clean. This section focuses on **detecting, handling, and transforming messy data** using Pandas. You will learn to:\n",
        "- Detect and handle missing or inconsistent data.\n",
        "- Standardize column names and datatypes.\n",
        "- Remove duplicates and outliers.\n",
        "- Apply transformations and mappings.\n",
        "- Prepare data for downstream analysis or machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Subtopics\n",
        "1. Identifying and handling missing values (`isna`, `fillna`, `dropna`).\n",
        "2. Removing duplicates and inconsistent rows.\n",
        "3. Data type conversions (`astype`, `to_datetime`).\n",
        "4. Feature creation and conditional column transformations.\n",
        "5. Outlier detection and trimming.\n",
        "6. End-to-end cleaning workflow example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "\n",
        "# Simulate a messy e-commerce dataset\n",
        "data = {\n",
        "    'CustomerID': [201, 202, 203, 204, 205, 205, None],\n",
        "    'Gender': ['Male', 'female', 'FEMALE', 'Male', np.nan, 'male', 'unknown'],\n",
        "    'Age': [25, 34, np.nan, 29, 999, 28, 30],\n",
        "    'PurchaseAmount': [250.0, np.nan, 100.5, 400.0, 500.0, 400.0, np.nan],\n",
        "    'JoinDate': ['2023-01-01', '2023/01/15', '01-02-2023', '2023-01-22', 'not_a_date', '2023-01-22', '2023-02-10']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ Detecting Missing Data\n",
        "\n",
        "Pandas provides vectorized utilities for detecting missing data. Use:\n",
        "- `df.isna()` ‚Üí boolean mask\n",
        "- `df.isna().sum()` ‚Üí missing count per column\n",
        "- `df.dropna()` or `df.fillna()` for imputation/removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('Missing values per column:')\n",
        "print(df.isna().sum(), '\\n')\n",
        "\n",
        "# Drop rows with missing CustomerID\n",
        "df = df.dropna(subset=['CustomerID'])\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ Handling Duplicates\n",
        "\n",
        "Duplicates often arise due to multiple data merges or logging events twice. Use:\n",
        "- `df.duplicated()` to detect.\n",
        "- `df.drop_duplicates()` to remove."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('Duplicate rows:')\n",
        "print(df[df.duplicated()])\n",
        "\n",
        "df = df.drop_duplicates()\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ Standardizing Columns and Data Types\n",
        "\n",
        "Column names often contain inconsistent cases or spaces. Use `.str.lower()` and `.str.replace()` for standardization. \n",
        "We also convert date-like and numeric fields properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Normalize column names\n",
        "df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "# Convert JoinDate column safely to datetime\n",
        "df['joindate'] = pd.to_datetime(df['joindate'], errors='coerce')\n",
        "\n",
        "# Convert Age to numeric and fix unrealistic values (>120 treated as missing)\n",
        "df.loc[df['age'] > 120, 'age'] = np.nan\n",
        "df['age'] = df['age'].astype('float')\n",
        "\n",
        "df.info()\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Handling Categorical and Text Data\n",
        "\n",
        "Standardize text categories using `.str.lower()`, `.replace()`, and `.map()`. \n",
        "For example, unify `Gender` values like `'male'`, `'Male'`, `'FEMALE'` ‚Üí `'Male'` / `'Female'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df['gender'] = df['gender'].str.lower().replace({\n",
        "    'female': 'Female',\n",
        "    'male': 'Male',\n",
        "    'unknown': np.nan\n",
        "})\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ Filling Missing Values and Creating Derived Columns\n",
        "\n",
        "- Numeric columns ‚Üí fill with mean, median, or domain logic.\n",
        "- Categorical columns ‚Üí fill with mode or forward-fill.\n",
        "- Derived columns ‚Üí computed from others (e.g., spending category)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df['age'] = df['age'].fillna(df['age'].median())\n",
        "df['purchaseamount'] = df['purchaseamount'].fillna(df['purchaseamount'].median())\n",
        "\n",
        "# Create spending category\n",
        "df['spend_level'] = pd.cut(df['purchaseamount'], bins=[0, 200, 400, 600], labels=['Low', 'Medium', 'High'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6Ô∏è‚É£ Outlier Detection (Simple Example)\n",
        "\n",
        "Detect outliers using IQR or z-score thresholds. Here we apply IQR method on `PurchaseAmount`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Q1 = df['purchaseamount'].quantile(0.25)\n",
        "Q3 = df['purchaseamount'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "outlier_mask = (df['purchaseamount'] < (Q1 - 1.5 * IQR)) | (df['purchaseamount'] > (Q3 + 1.5 * IQR))\n",
        "df_outliers = df[outlier_mask]\n",
        "\n",
        "print('Detected outliers:')\n",
        "df_outliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clean final dataset after trimming outliers\n",
        "Often, trimming top/bottom 1% can improve model stability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_clean = df[~outlier_mask].copy()\n",
        "df_clean.reset_index(drop=True, inplace=True)\n",
        "df_clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Real-World Problem 1 ‚Äî Cleaning Survey Data\n",
        "\n",
        "**Scenario:** You receive a CSV with inconsistent gender labels, missing ages, and invalid entries like 0 or 999. \n",
        "You must clean and standardize it for demographic analysis.\n",
        "\n",
        "**Steps to demonstrate:**\n",
        "1. Identify invalid/missing entries.\n",
        "2. Standardize case and category names.\n",
        "3. Impute missing ages with median.\n",
        "4. Remove unrealistic values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "survey = pd.DataFrame({\n",
        "    'RespondentID': [1,2,3,4,5,6],\n",
        "    'Gender': ['male', 'Female', 'FEMALE', 'Unknown', 'male', 'None'],\n",
        "    'Age': [22, 0, 30, 27, 999, np.nan]\n",
        "})\n",
        "\n",
        "# Step 1 & 2\n",
        "survey['Gender'] = survey['Gender'].str.lower().replace({'female': 'Female', 'male': 'Male', 'unknown': np.nan, 'none': np.nan})\n",
        "\n",
        "# Step 3 & 4\n",
        "survey.loc[(survey['Age'] <= 0) | (survey['Age'] > 120), 'Age'] = np.nan\n",
        "survey['Age'] = survey['Age'].fillna(survey['Age'].median())\n",
        "\n",
        "survey"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Real-World Problem 2 ‚Äî Transaction Data Preprocessing for ML\n",
        "\n",
        "**Goal:** Prepare transaction data for machine learning model input.\n",
        "\n",
        "1. Impute missing numerical features.\n",
        "2. Encode categories.\n",
        "3. Scale numerical columns (simple min-max normalization)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "transactions = pd.DataFrame({\n",
        "    'CustomerID': [101, 102, 103, 104, 105],\n",
        "    'Gender': ['Male', 'Female', np.nan, 'Male', 'Female'],\n",
        "    'SpendingScore': [45, np.nan, 70, 30, 100]\n",
        "})\n",
        "\n",
        "# Fill missing categorical and numeric values\n",
        "transactions['Gender'] = transactions['Gender'].fillna(transactions['Gender'].mode()[0])\n",
        "transactions['SpendingScore'] = transactions['SpendingScore'].fillna(transactions['SpendingScore'].median())\n",
        "\n",
        "# Encode categories\n",
        "transactions['Gender'] = transactions['Gender'].map({'Male': 0, 'Female': 1})\n",
        "\n",
        "# Normalize SpendingScore\n",
        "transactions['SpendingScore_scaled'] = (transactions['SpendingScore'] - transactions['SpendingScore'].min()) / (transactions['SpendingScore'].max() - transactions['SpendingScore'].min())\n",
        "\n",
        "transactions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Under the Hood: Pandas Cleaning Internals\n",
        "- `fillna()` and `dropna()` are vectorized; they modify DataFrame blocks directly for efficiency.\n",
        "- String operations (`.str`) use underlying NumPy ufuncs or optimized Cythonized loops.\n",
        "- `to_datetime(errors='coerce')` silently converts invalid dates to `NaT` without breaking pipelines.\n",
        "- `astype()` triggers type inference cascades for mixed dtypes.\n",
        "- Operations like `cut` and `qcut` internally rely on `np.searchsorted()` for binning efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí° Best Practices / Common Pitfalls\n",
        "- Always inspect missing data with `df.isna().sum()` before cleaning.\n",
        "- Never use inplace fills (`inplace=True`) in production ‚Äî they break chainability.\n",
        "- When imputing, use domain knowledge (e.g., median per group).\n",
        "- Keep a log of data-cleaning rules for reproducibility.\n",
        "- Normalize text case early to avoid duplicates in grouping or joining."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß© Challenge Exercise\n",
        "\n",
        "You‚Äôre given a dataset of customers with columns: `ID`, `Gender`, `Age`, `Income`, and `SignupDate`. The dataset contains missing, invalid, and inconsistent entries.\n",
        "\n",
        "**Tasks:**\n",
        "1. Clean missing and invalid `Age` and `Income` values.\n",
        "2. Normalize gender values (`male`, `M`, `female`, `F`, etc.).\n",
        "3. Convert `SignupDate` into proper datetime format.\n",
        "4. Create a new column `IncomeLevel` using quantile binning.\n",
        "5. Drop duplicates and output final clean dataset.\n",
        "\n",
        "_Hint:_ Use `pd.qcut()` for quantile-based income bins and `replace()` for category mapping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# --- End of Section 3 ‚Äî Continue to Section 4 ---"
      ]
    }
  ]
}
